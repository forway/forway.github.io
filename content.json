{"pages":[{"title":"","text":"kMrK6mp0iB","link":"/baidu_verify_kMrK6mp0iB.html"},{"title":"ABOUT","text":"专注研究软件开发、机器学习","link":"/about/index.html"}],"posts":[{"title":"elasticsearch-集群安装","text":"这里假如有两个节点：node-1、node-2配置：elasticsearch.yml node-1: 12345678cluster.name: liangnode.name: node-1bootstrap.mlockall: truenetwork.host: masternetwork.publish_host: masterdiscovery.zen.ping.unicast.hosts: [&quot;slave1&quot;]path.data: /usr/local/install_soft/elasticsearch-2.3.4/datapath.logs: /usr/local/install_soft/elasticsearch-2.3.4/logs node-2: 12345678cluster.name: liangnode.name: node-2bootstrap.mlockall: truenetwork.host: slave1network.publish_host: slave1discovery.zen.ping.unicast.hosts: [&quot;master&quot;]path.data: /usr/local/install_soft/elasticsearch-2.3.4/datapath.logs: /usr/local/install_soft/elasticsearch-2.3.4/logs 注意：1、所有的配置前面空一个空字符2、配置格式 key: value，value前面空一个空字符如： 1cluster.name: sbt_general_app 扩展:只需要修改discovery.zen.ping.unicast.hosts属性，其内的值不需要是全部运行节点的ip，只需要任意一台运行中的IP即可","link":"/2019/12/28/elesticsearch-集群安装/"},{"title":"linux-常用命令","text":"1、查找文件 12find / -name httpd.conffind /etc -name &apos;*srm*&apos; 2、设置自动任务系统级别： 12vi /etc/crontab0 1 * * * sh /a.sh 用户级别： 12crontab -e -u root0 1 * * * sh /a.sh 检查开机启动： 12chkconfig --list crondcrontab -l -u user 时间任务格式： 12*/1 * * * * sh /home/hadoop/test2/start_offline.sh0 1 * * * sh /home/hadoop/test2/start_offline.sh 3、文本替换 vi/vim 中可以使用 ：s 命令来替换字符串。该命令有很多种不同细节使用方法，可以实现复杂的功能，记录几种在此，方便以后查询。 1234567 ：s/vivian/sky/ 替换当前行第一个 vivian 为 sky ：s/vivian/sky/g 替换当前行所有 vivian 为 sky ：n，$s/vivian/sky/ 替换第 n 行开始到最后一行中每一行的第一个 vivian 为 sky ：n，$s/vivian/sky/g 替换第 n 行开始到最后一行中每一行所有 vivian 为 sky n 为数字，若 n 为 .，表示从当前行开始到最后一行 123 ：%s/vivian/sky/（等同于 ：g/vivian/s//sky/） 替换每一行的第一个 vivian 为 sky ：%s/vivian/sky/g（等同于 ：g/vivian/s//sky/g） 替换每一行中所有 vivian 为 sky 可以使用 # 作为分隔符，此时中间出现的 / 不会作为分隔符 123 ：s#vivian/#sky/# 替换当前行第一个 vivian/ 为 sky/ ：%s+/oradata/apras/+/user01/apras1+ （使用+ 来 替换 / ）： /oradata/apras/替换成/user01/apras1/ 123 1.：s/vivian/sky/ 替换当前行第一个 vivian 为 sky ：s/vivian/sky/g 替换当前行所有 vivian 为 sky 12345 2. ：n，$s/vivian/sky/ 替换第 n 行开始到最后一行中每一行的第一个 vivian 为 sky ：n，$s/vivian/sky/g 替换第 n 行开始到最后一行中每一行所有 vivian 为 sky （n 为数字，若 n 为 .，表示从当前行开始到最后一行） 123 3. ：%s/vivian/sky/（等同于 ：g/vivian/s//sky/） 替换每一行的第一个 vivian 为 sky ：%s/vivian/sky/g（等同于 ：g/vivian/s//sky/g） 替换每一行中所有 vivian 为 sky 123 4. 可以使用 # 作为分隔符，此时中间出现的 / 不会作为分隔符 ：s#vivian/#sky/# 替换当前行第一个 vivian/ 为 sky/ 1234567891011121314151617181920 5. 删除文本中的^M 问题描述：对于换行，window下用回车换行（0A0D）来表示，linux下是回车（0A）来表示。这样，将window上的文件拷到unix上用时，总会有个^M.请写个用在unix下的过滤windows文件的换行符（0D）的shell或c程序。 。 使用命令：cat filename1 | tr -d “^V^M” &gt; newfile； 。 使用命令：sed -e “s/^V^M//” filename &gt; outputfilename.需要注意的是在1、2两种方法中，^V和^M指的是Ctrl+V和Ctrl+M.你必须要手工进行输入，而不是粘贴。 。 在vi中处理：首先使用vi打开文件，然后按ESC键，接着输入命令：%s/^V^M//. 。 ：%s/^M$//g 如果上述方法无用，则正确的解决办法是： [Page] 。 tr -d \\&quot;\\\\r\\&quot; &lt; src &gt;dest 。 tr -d \\&quot;\\\\015\\&quot; dest 。 strings A&gt;B 1234566. 替换确认我们有很多时候会需要某个字符(串)在文章中某些位置出现时被替换，而其它位置不被替换的有选择的操作，这就需要用户来进行确认，vi的查找替换同样支持例如：s/vivian/sky/g 替换当前行所有 vivian 为 sky 在命令后面加上一个字母c就可以实现，即：s/vivian/sky/gc顾名思意，c是confirm的缩写 12345678910111213 7. 其它 利用 ：s 命令可以实现字符串的替换。具体的用法包括： ：s/str1/str2/ 用字符串 str2 替换行中首次出现的字符串 str1 ：s/str1/str2/g 用字符串 str2 替换行中所有出现的字符串 str1 ：。，$ s/str1/str2/g 用字符串 str2 替换正文当前行到末尾所有出现的字符串 str1 ：1，$ s/str1/str2/g 用字符串 str2 替换正文中所有出现的字符串 str1 ：g/str1/s//str2/g 功能同上 从上述替换命令可以看到：g 放在命令末尾，表示对搜索字符串的每次出现进行替换；不加 g，表示只对搜索 字符串的首次出现进行替换；g 放在命令开头，表示对正文中所有包含搜索字符串的行进行替换操作 4、用户线程数设置 Linux对于每个用户，系统限制其最大进程数。为提高性能，可以根据设备资源情况，设置各linux 用户的最大进程数可以用ulimit -a 来显示当前的各种用户进程限制。下面我把某linux用户的最大进程数设为10000个： 12345678910ulimit -u 10240对于需要做许多 socket 连接并使它们处于打开状态的 Java 应用程序而言，最好通过使用 ulimit -n xx 修改每个进程可打开的文件数，缺省值是 1024。ulimit -n 4096 将每个进程可以打开的文件数目加大到4096，缺省为1024其他建议设置成无限制（unlimited）的一些重要设置是：数据段长度：ulimit -d unlimited最大内存大小：ulimit -m unlimited堆栈大小：ulimit -s unlimitedCPU 时间：ulimit -t unlimited虚拟内存：ulimit -v unlimited 暂时地，适用于通过 ulimit 命令登录 shell 会话期间。 永久地，通过将一个相应的 ulimit 语句添加到由登录 shell 读取的文件中， 即特定于 shell 的用户资源文件，如：1)、解除 Linux 系统的最大进程数和最大文件打开数限制： 1234567 vi /etc/security/limits.conf # 添加如下的行 * soft noproc 11000 * hard noproc 11000 * soft nofile 4100 * hard nofile 4100说明：* 代表针对所有用户，noproc 是代表最大进程数，nofile 是代表最大文件打开数 2)、让 SSH 接受 Login 程式的登入，方便在 ssh 客户端查看 ulimit -a 资源限制： 1234a、vi /etc/ssh/sshd_config 把 UserLogin 的值改为 yes，并把 # 注释去掉 b、重启 sshd 服务： /etc/init.d/sshd restart 3)、修改所有 linux 用户的环境变量文件： 12345678vi /etc/profile ulimit -u 10000 ulimit -n 4096 ulimit -d unlimited ulimit -m unlimited ulimit -s unlimited ulimit -t unlimited ulimit -v unlimited 保存后运行#source /etc/profile 使其生效 可以用ulimit -a 来显示当前的各种用户进程限制 参数 描述ulimited 不限制用户可以使用的资源，但本设置对可打开的最大文件数（max open files）和可同时运行的最大进程数（max user processes）无效 1234567891011121314-a 列出所有当前资源极限-c 设置core文件的最大值.单位:blocks-d 设置一个进程的数据段的最大值.单位:kbytes-f Shell 创建文件的文件大小的最大值，单位：blocks-h 指定设置某个给定资源的硬极限。如果用户拥有 root 用户权限，可以增大硬极限。任何用户均可减少硬极限-l 可以锁住的物理内存的最大值-m 可以使用的常驻内存的最大值,单位：kbytes-n 每个进程可以同时打开的最大文件数-p 设置管道的最大值，单位为block，1block=512bytes-s 指定堆栈的最大值：单位：kbytes-S 指定为给定的资源设置软极限。软极限可增大到硬极限的值。如果 -H 和 -S 标志均未指定，极限适用于以上二者-t 指定每个进程所使用的秒数,单位：seconds-u 可以运行的最大并发进程数-v Shell可使用的最大的虚拟内存，单位：kbytes","link":"/2019/12/28/linux-常用命令/"},{"title":"flume-拦截器详解","text":"拦截器介绍一个拦截器的功能可以被概括为这个方法： public Event intercept(Event event); 它传入一个event然后返回一个event。它可以不做任何事情；也就是说没有任何改变的event将被返回。通常来说，它用一些有用的方式来改变event的内容。如果返回null值，这个event就表示被丢弃了。 要往source上添加拦截器，只需要添加interceptors属性到该source上。例如： 1agent.sources.s1.interceptors=i1 i2 i3 这里在名为s1的source上定义了三个拦截器，i1、i2和i3 【拦截器按他们的排列顺序运行。在前面这个例子中，i2将接收i1的输出。I3将接收i2的输出。最后channel选择器接收i3的输出。】 现在我们已经定义了拦截器的名字，我们需要通过如下方式指定他们的类型： 1234agent.sources.s1.interceptors.i1.type=TYPE1 agent.sources.s1.interceptors.i1.additionalProperty1=VALUEagent.sources.s1.interceptors.i2.type=TYPE2 agent.sources.s1.interceptors.i3.type=TYPE3 让我们看看它们中的一些拦截器，由Flume自带的，来看看有什么更好的方式来配置他们。 (1) Timestamp拦截器Timestamp拦截器，正如它名字暗示的，在Flume event的header中添加一个timestamp的key，如果这个key值还不存在的话。要使用它，设置type属性值为timestamp即可。如果event的header中已经包含了一个timestamp的key，它将用当前时间来覆盖这个值，除非你设置preserveExisting属性的值为true来保护原有的值。 如果我们只想在header中不存在timestamp时添加该属性，那么这里有一个对source来说看起来相对完整的拦截器配置： 123agent.sources.s1.interceptors=i1 agent.sources.s1.interceptors.i1.type=timestamp agent.sources.s1.interceptors.i1.preserveExisting=true (2) Host拦截器和Timestamp拦截器基本类似，Host拦截器将在当前Flume agent 的event的header中添加IP地址。要使用它，设置type属性为host即可。agent.sources.s1.interceptors=i1agent.sources.s1.interceptors.type=host 如果我们只想在header中添加一个relayHost的key，通过DNS反向查找得到的hostname作为value值，那么这里有一个对source来说看起来相对完整的拦截器配置： 1234agent.sources.s1.interceptors=i1 agent.sources.s1.interceptors.i1.type=host agent.sources.s1.interceptors.i1.hostHeader=relayHost agent.sources.s1.interceptors.i1.useIP=false (3) Static拦截器Static拦截器是用来往经过的所有Flume event的header中插入单个任何形式的key/value的。如果需要插入多个key/value值，你只需要添加更多的static拦截器即可。不像我们已经见过的其他拦截器，默认情况下该拦截器会保留已经存在的key的值。就像以往的一样，我建议你指定具体的值而非依赖默认值 最后，让我们看一下假设在先前不存在这两个值的情况下插入两个新的header值的例子： 1234567agent.sources.s1.interceptors=pos env agent.sources.s1.interceptors.pos.type=static agent.sources.s1.interceptors.pos.key=pointOfSale agent.sources.s1.interceptors.pos.value=US agent.sources.s1.interceptors.env.type=static agent.sources.s1.interceptors.env.key=environment agent.sources.s1.interceptors.env.value=staging (4) 正则表达式过滤拦截器如果你想要依据body的内容来过滤event，正则表达式过滤拦截器对你来说很有用。根据你提供的正则表达式，它将会筛选出符合条件的event或者只保留符合条件的event。让我们从设置type属性值为regex_filter开始。你想匹配的表达需要使用Java格式的正则表达式来书写。 正则表达式字符串是在regex属性中设置的。最后，你需要通过设置excludeEvents属性为true告诉拦截器你想要排除匹配正则表达式的event。默认值（false）说明你只想要可以匹配正则表达式的event。 12345在这个例子中，任何包含“NullPointerException”字符串的event都会被丢弃掉：agent.sources.s1.interceptors=npe agent.sources.s1.interceptors.npe.type=regex_filter agent.sources.s1.interceptors.npe.regex=NullPointerException agent.sources.s1.interceptors.npe.excludeEvents=true (5) 正则表达式提取拦截器有时候你想要提取event body中的部分二进制数据到Flume header中，这样你就可以通过channel选择区来路由event了。你可以使用正则表达式提取拦截器来完成这个操作。我们通过设置type属性值为regex_extractor开始： 12agent.sources.s1.interceptors=e1 agent.sources.s1.interceptors.e1.type=regex_extractor 像正则表达式过滤拦截器一样，正则表达式提取拦截器也使用Java格式的正则表达式来书写。为了提取一到多个片段，你需要从设置regex属性值为一组匹配的括号开始。让我们假设我们想要查找出event中的错误数量，格式为“Error:N”（N为数字）： 123agent.sources.s1.interceptors=e1 agent.sources.s1.interceptors.e1.type=regex_extractor agent.sources.s1.interceptors.e1.regex=Error:\\\\s(\\\\d+) 正如你看到的我把捕获数字的正则放在括号中，可以包含一到多个数字。现在我可以匹配我想要的表达式，我需要告诉Flume我应该用这个表达式做什么。这里我们需要介绍下serializers ，它提供了一种可插拔的机制去解释每个表达式。在这个例子中我只需要一个匹配值，所以我用空格分隔的序列化列表只有一个值：【注意java格式的正则表达式中，符号 \\ 需要转义：\\】 123456agent.sources.s1.interceptors=e1 agent.sources.s1.interceptors.e1.type=regex_extractor agent.sources.s1.interceptors.e1.regex=Error:\\\\s(\\\\d+) agent.sources.s1.interceptors.e1.serializers=ser1 agent.sources.s1.interceptors.e1.serializers.ser1.type=default //可以不要这个默认设置 agent.sources.s1.interceptors.e1.serializers.ser1.name=error_no name属性指定了event的key值，而value的内容则用来匹配正则表达式。Type属性的值为default（如果没有指定默认也是default）是一个简单的直接通过的序列化器。比如下面的event body：NullPointerException: A problem occurred. Error: 123. TxnID: 5X2T9E. 下面的header将会被添加到event中： 1{ &quot;error_no&quot;:&quot;123&quot; } 如果我想把TxnID的值添加到header中，我仅仅需要增加另一组正则匹配和序列化器： 12345678agent.sources.s1.interceptors=e1 agent.sources.s1.interceptors.e1.type=regex_extractor agent.sources.s1.interceptors.e1.regex=Error:\\\\s(\\\\d+).*TxnID:\\\\s(\\\\w+)agent.sources.s1.interceptors.e1.serializers=ser1 ser2 agent.sources.s1.interceptors.e1.serializers.ser1.type=default agent.sources.s1.interceptors.e1.serializers.ser1.name=error_no agent.sources.s1.interceptors.e1.serializers.ser2.type=default //可以不要这个默认设置agent.sources.s1.interceptors.e1.serializers.ser2.name=txnid 对于前面的输入内容，将会加上如下的header值：{ “error_no”:”123”， “txnid”:”5x2T9E” } 然而，如果内容颠倒了，像这样：NullPointerException: A problem occurred. TxnID: 5X2T9E. Error: 123.我最终只会得到TxnID这一个header。一个处理这种情形更好的方式就是使用多重拦截器这样命令就不会有问题了： 1234567891011agent.sources.s1.interceptors=e1 e2 agent.sources.s1.interceptors.e1.type=regex_extractor agent.sources.s1.interceptors.e1.regex=Error:\\\\s(\\\\d+) agent.sources.s1.interceptors.e1.serializers=ser1 agent.sources.s1.interceptors.e1.serializers.ser1.type=default agent.sources.s1.interceptors.e1.serializers.ser1.name=error_no agent.sources.s1.interceptors.e2.type=regex_extractor agent.sources.s1.interceptors.e2.regex=TxnID:\\\\s(\\\\w+) agent.sources.s1.interceptors.e2.serializers=ser1 agent.sources.s1.interceptors.e2.serializers.ser1.type=default agent.sources.s1.interceptors.e2.serializers.ser1.name=txnid 除了直接通过的default序列化器，Flume系统自带的另外唯一一个序列化实现就是需要指定全局限定类名的org.apache.flume.interceptor.RegexExtractorInterceptorMillisSerializer。这个序列化器用来将时间转化回毫秒的。你需要基于org.joda.time.format.DateTimeFormat正则来指定对应的正则表达式属性。 比如，我们假设你正在提取Apache网站服务器的日志。例如：192.168.1.42 - - [29/Mar/2013:15:27:09 -0600] “GET /index.html HTTP/1.1” 200 1037对这个数据完整的正则表达式可能像这样（以Java字符串的形式，带有反斜杠和用反斜杠转义的引号）： 1^([\\\\d.]+) \\\\S+ \\\\S+ \\\\[([\\\\w:/]+\\\\s[+\\\\-]\\\\d{4})\\\\] \\&quot;(.+?)\\&quot; (\\\\d{3}) (\\\\d+) 这个时间正则表达式完全符合org.joda.time.format.DateTimeFormat正则的形式：yyyy/MMM/dd:HH:mm:ss Z 这会让我们的配置文件变得和下面这个片段类似： 1234567891011agent.sources.s1.interceptors=e1 agent.sources.s1.interceptors.e1.type=regex_extractor agent.sources.s1.interceptors.e1.regex=^([\\\\d.]+) \\\\S+ \\\\S+ \\\\ [([\\\\w:/]+\\\\s[+\\\\-]\\\\d{4})\\\\] \\&quot;(.+?)\\&quot; (\\\\d{3}) (\\\\d+)agent.sources.s1.interceptors.e1.serializers=ip dt url sc bc agent.sources.s1.interceptors.e1.serializers.ip.name=ip_address agent.sources.s1.interceptors.e1.serializers.dt.type=org.apache.flume. interceptor.RegexExtractorInterceptorMillisSerializeragent.sources.s1.interceptors.e1.serializers.dt.pattern=yyyy/MMM/ dd:HH:mm:ss Z agent.sources.s1.interceptors.e1.serializers.dt.name=timestamp agent.sources.s1.interceptors.e1.serializers.url.name=http_requestagent.sources.s1.interceptors.e1.serializers.sc.name=status_code agent.sources.s1.interceptors.e1.serializers.bc.name=bytes_xfered 这将对前文的内容产生如下的header信息：{ “ip_address”:”192.168.1.42”， “timestamp”:”1364588829”， “http_request”:”GET /index.html HTTP/1.1”， “status_code”:”200”， “bytes_xfered”:”1037” } 【在拦截器中没有重写检查。例如，使用timestamp作为key值将会覆盖event之前存在的时间，如果存在的话】 (6) 自定义拦截器如果你有你点想要添加进你Flume实现中的自定义代码，它很大可能是一个自定义拦截器。如之前提到的，你需要实现org.apache.flume.interceptor.Interceptor接口以及关联的org.apache.flume.interceptor.Interceptor.Builder接口。假设我需要对我event的body的内容进行URL编码。代码可能如下方所示： 123456789101112131415161718192021222324252627282930313233public class URLDecode implements Interceptor { public void initialize() { } public Event intercept(Event event) { try { byte[] decoded = URLDecoder.decode(new String(event.getBody())， \"UTF-8\").getBytes(\"UTF-8\"); event.setBody(decoded); } catch UnsupportedEncodingException e) { // This shouldn't happen. Fall through to unaltered event. } return event; } public List&lt;Event&gt; intercept(List&lt;Event&gt; events) { for (Event event:events) { intercept(event); } return events; } public void close() { } public static class Builder implements Interceptor.Builder { public Interceptor build() { return new URLDecode(); } public void configure(Context context) { } }} 然后如果要配置我的新拦截器，对Builder类使用类似如下格式的FQDN（完全限定域名）来作为type属性的值： 12agent.sources.s1.interceptors=i1 agent.sources.s1.interceptors.i1.type=com.example.URLDecoder$Builder 对于如何传递和验证属性值的例子，可以去看Flume source部分已经实现的拦截器的源码来寻找灵感。请记住自定义拦截器中任何繁重的处理过程都将影响整体的吞吐量，所以请注意自定义拦截器中对象的生成和密集的计算工作。","link":"/2019/12/28/flume-拦截器详解/"},{"title":"编程机器人游戏攻略","text":"这里给程序猿们推荐一款和编程有关的小游戏：编程机器人或点灯机器人，个人觉得非常不错，玩这个游戏的关键是找出相似路线，把相似路线封装成方法不断调用，下面是不同关卡的攻略，哪位高手有更好的方法，欢迎指教。","link":"/2019/12/28/game-编程机器人游戏攻略/"},{"title":"spark-schedule性能优化","text":"调度相关的参数设置，大多数内容都很直白，其实无须过多的额外解释，不过基于这些参数的常用性（大概会是你针对自己的集群第一步就会配置的参数），这里多少就其内部机制做一些解释。 schedule性能优化参数设置(1) spark.cores.max一个集群最重要的参数之一，当然就是CPU计算资源的数量。spark.cores.max 这个参数决定了在Standalone和Mesos模式下，一个Spark应用程序所能申请的CPU Core的数量。如果你没有并发跑多个Spark应用程序的需求，那么可以不需要设置这个参数，默认会使用spark.deploy.defaultCores的值（而spark.deploy.defaultCores的值默认为Int.Max，也就是不限制的意思）从而应用程序可以使用所有当前可以获得的CPU资源。 针对这个参数需要注意的是，这个参数对Yarn模式不起作用，YARN模式下，资源由Yarn统一调度管理，一个应用启动时所申请的CPU资源的数量由另外两个直接配置Executor的数量和每个Executor中core数量的参数决定。（历史原因造成，不同运行模式下的一些启动参数个人认为还有待进一步整合） 此外，在Standalone模式等后台分配CPU资源时，目前的实现中，在spark.cores.max允许的范围内，基本上是优先从每个Worker中申请所能得到的最大数量的CPU core给每个Executor，因此如果人工限制了所申请的Max Core的数量小于Standalone和Mesos模式所管理的CPU数量，可能发生应用只运行在集群中部分节点上的情况（因为部分节点所能提供的最大CPU资源数量已经满足应用的要求），而不是平均分布在集群中。通常这不会是太大的问题，但是如果涉及数据本地性的场合，有可能就会带来一定的必须进行远程数据读取的情况发生。理论上，这个问题可以通过两种途径解决：一是Standalone和Mesos的资源管理模块自动根据节点资源情况，均匀分配和启动Executor，二是和Yarn模式一样，允许用户指定和限制每个Executor的Core的数量。 社区中有一个PR试图走第二种途径来解决类似的问题，不过截至我写下这篇文档为止（2014.8），还没有被Merge。 (2) spark.task.cpus这个参数在字面上的意思就是分配给每个任务的CPU的数量，默认为1。实际上，这个参数并不能真的控制每个任务实际运行时所使用的CPU的数量，比如你可以通过在任务内部创建额外的工作线程来使用更多的CPU（至少目前为止，将来任务的执行环境是否能通过LXC等技术来控制还不好说）。它所发挥的作用，只是在作业调度时，每分配出一个任务时，对已使用的CPU资源进行计数。也就是说只是理论上用来统计资源的使用情况，便于安排调度。因此，如果你期望通过修改这个参数来加快任务的运行，那还是赶紧换个思路吧。这个参数的意义，个人觉得还是在你真的在任务内部自己通过任何手段，占用了更多的CPU资源时，让调度行为更加准确的一个辅助手段。 (3) spark.scheduler.mode这个参数决定了单个Spark应用内部调度的时候使用FIFO模式还是Fair模式。是的，你没有看错，这个参数只管理一个Spark应用内部的多个没有依赖关系的Job作业的调度策略。 如果你需要的是多个Spark应用之间的调度策略，那么在Standalone模式下，这取决于每个应用所申请和获得的CPU资源的数量（暂时没有获得资源的应用就Pending在那里了），基本上就是FIFO形式的，谁先申请和获得资源，谁就占用资源直到完成。而在Yarn模式下，则多个Spark应用间的调度策略由Yarn自己的策略配置文件所决定。 那么这个内部的调度逻辑有什么用呢？如果你的Spark应用是通过服务的形式，为多个用户提交作业的话，那么可以通过配置Fair模式相关参数来调整不同用户作业的调度和资源分配优先级。 (4) spark.locality.wait这几个参数影响了任务分配时的本地性策略的相关细节： spark.locality.wait spark.locality.wait.process spark.locality.wait.node spark.locality.wait.rackSpark中任务的处理需要考虑所涉及的数据的本地性的场合，基本就两种，一是数据的来源是HadoopRDD; 二是RDD的数据来源来自于RDD Cache（即由CacheManager从BlockManager中读取，或者Streaming数据源RDD）。其它情况下，如果不涉及shuffle操作的RDD，不构成划分Stage和Task的基准，不存在判断Locality本地性的问题，而如果是ShuffleRDD，其本地性始终为No Prefer，因此其实也无所谓Locality。 在理想的情况下，任务当然是分配在可以从本地读取数据的节点上时（同一个JVM内部或同一台物理机器内部）的运行时性能最佳。但是每个任务的执行速度无法准确估计，所以很难在事先获得全局最优的执行策略，当Spark应用得到一个计算资源的时候，如果没有可以满足最佳本地性需求的任务可以运行时，是退而求其次，运行一个本地性条件稍差一点的任务呢，还是继续等待下一个可用的计算资源已期望它能更好的匹配任务的本地性呢？ 这几个参数一起决定了Spark任务调度在得到分配任务时，选择暂时不分配任务，而是等待获得满足进程内部/节点内部/机架内部这样的不同层次的本地性资源的最长等待时间。默认都是3000毫秒。 基本上，如果你的任务数量较大和单个任务运行时间比较长的情况下，单个任务是否在数据本地运行，代价区别可能比较显著，如果数据本地性不理想，那么调大这些参数对于性能优化可能会有一定的好处。反之如果等待的代价超过带来的收益，那就不要考虑了。 特别值得注意的是：在处理应用刚启动后提交的第一批任务时，由于当作业调度模块开始工作时，处理任务的Executors可能还没有完全注册完毕，因此一部分的任务会被放置到No Prefer的队列中，这部分任务的优先级仅次于数据本地性满足Process级别的任务，从而被优先分配到非本地节点执行，如果的确没有Executors在对应的节点上运行，或者的确是No Prefer的任务（如shuffleRDD），这样做确实是比较优化的选择，但是这里的实际情况只是这部分Executors还没来得及注册上而已。这种情况下，即使加大本节中这几个参数的数值也没有帮助。针对这个情况，有一些已经完成的和正在进行中的PR通过例如动态调整No Prefer队列，监控节点注册比例等等方式试图来给出更加智能的解决方案。不过，你也可以根据自身集群的启动情况，通过在创建SparkContext之后，主动Sleep几秒的方式来简单的解决这个问题。 (5) spark.speculation以下等参数调整Speculation行为的具体细节，Speculation是在任务调度的时候， 如果没有适合当前本地性要求的任务可供运行，将跑得慢的任务在空闲计算资源上 再度调度的行为，这些参数调整这些行为的频率和判断指标，默认是不使用Speculation的 ： spark.speculation spark.speculation.interval spark.speculation.quantile spark.speculation.multiplier通常来说很难正确的判断是否需要Speculation，能真正发挥Speculation用处的场合，往往是某些节点由于运行环境原因，比如CPU资源由于某种原因被占用，磁盘损坏导致IO缓慢造成任务执行速度异常的情况，当然前提是你的分区任务不存在仅能被执行一次，或者不能同时执行多个拷贝等情况。Speculation任务参照的指标通常是其它任务的执行时间，而实际的任务可能由于分区数据尺寸不均匀，本来就会有时间差异，加上一定的调度和IO的随机性，所以如果一致性指标定得过严，Speculation可能并不能真的发现问题，反而增加了不必要的任务开销，定得过宽，大概又基本相当于没用。 个人觉得，如果你的集群规模比较大，运行环境复杂，的确可能经常发生执行异常，加上数据分区尺寸差异不大，为了程序运行时间的稳定性，那么可以考虑仔细调整这些参数。否则还是考虑如何排除造成任务执行速度异常的因数比较靠铺一些。 当然，我没有实际在很大规模的集群上运行过Spark，所以如果看法有些偏颇，还请有实际经验的XD指正。","link":"/2019/12/28/spark-schedule-optimization/"},{"title":"java-AES加解密和RSA加解密","text":"AES加解密（对称加密算法）高级加密标准（英语：Advanced Encryption Standard，缩写：AES），是一种区块加密标准。这个标准用来替代原先的DES，已经被多方分析且广为全世界所使用。那么为什么原来的DES会被取代呢，原因就在于其使用56位密钥，比较容易被破解。而AES可以使用128、192、和256位密钥，并且用128位分组加密和解密数据，相对来说安全很多。 完善的加密算法在理论上是无法破解的，除非使用穷尽法。使用穷尽法破解密钥长度在128位以上的加密数据是不现实的，仅存在理论上的可能性。统计显示，即使使用目前世界上运算速度最快的计算机，穷尽128位密钥也要花上几十亿年的时间，更不用说去破解采用256位密钥长度的AES算法了。注意：指定加密密码生成的秘钥都是一致的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256package com.systex.bh.utils;import static com.systex.bh.db.entity.Constant.AES_KEY;import java.security.Key;import java.security.NoSuchAlgorithmException;import java.security.SecureRandom;import javax.crypto.Cipher;import javax.crypto.KeyGenerator;import javax.crypto.SecretKey;import javax.crypto.spec.SecretKeySpec;import org.apache.commons.codec.binary.Hex;public class AESUtil{ /** * 密钥算法 */ private static final String KEY_ALGORITHM = \"AES\"; private static final String DEFAULT_CIPHER_ALGORITHM = \"AES/ECB/PKCS5Padding\"; /** * 指定字符串初始化密钥 ： * 利用randomInfo密码作为随机数初始化出128位的key生产者，加密没关系，SecureRandom是生成安全随机数序列， * randomInfo.getBytes()是种子，只要种子相同，序列就一样，所以解密只要有password就行 * * @param randomInfo 加密时的密码，可以指定 * @return byte[] 密钥 * @throws Exception */ public static byte[] getSecretKey(String randomInfo) { //返回生成指定算法的秘密密钥的 KeyGenerator 对象 KeyGenerator kg = null; try { kg = KeyGenerator.getInstance(KEY_ALGORITHM); //初始化此密钥生成器，使其具有确定的密钥大小 //AES 要求密钥长度为 128 if (null == randomInfo || \"\".equals(randomInfo)) { //不指定字符串生成秘钥 kg.init(128); } else { SecureRandom random = SecureRandom.getInstance(\"SHA1PRNG\"); random.setSeed(randomInfo.getBytes()); //指定字符串生成秘钥 kg.init(128, random); } } catch (NoSuchAlgorithmException e) { e.printStackTrace(); return new byte[0]; } //生成一个密钥 SecretKey secretKey = kg.generateKey(); return secretKey.getEncoded(); } /** * 不指定字符串初始化密钥 */ public static byte[] getSecretKey() { return getSecretKey(null); } /** * 转换密钥 * * @param key 二进制密钥 * @return 密钥 */ public static Key toKey(byte[] key) { //生成密钥 return new SecretKeySpec(key, KEY_ALGORITHM); } /** * 加密 * * @param data 待加密数据 * @param key 密钥 * @return byte[] 加密数据 * @throws Exception */ public static byte[] encrypt(byte[] data, Key key) throws Exception { return encrypt(data, key, DEFAULT_CIPHER_ALGORITHM); } /** * 加密 * * @param data 待加密数据 * @param key 二进制密钥 * @return byte[] 加密数据 * @throws Exception */ public static byte[] encrypt(byte[] data, byte[] key) throws Exception { return encrypt(data, key, DEFAULT_CIPHER_ALGORITHM); } /** * 加密 * * @param data 待加密数据 * @param key 二进制密钥 * @param cipherAlgorithm 加密算法/工作模式/填充方式 * @return byte[] 加密数据 * @throws Exception */ public static byte[] encrypt(byte[] data, byte[] key, String cipherAlgorithm) throws Exception { //还原密钥 Key k = toKey(key); return encrypt(data, k, cipherAlgorithm); } /** * 加密 * * @param data 待加密数据 * @param key 密钥 * @param cipherAlgorithm 加密算法/工作模式/填充方式 * @return byte[] 加密数据 * @throws Exception */ public static byte[] encrypt(byte[] data, Key key, String cipherAlgorithm) throws Exception { //实例化 Cipher cipher = Cipher.getInstance(cipherAlgorithm); //使用密钥初始化，设置为加密模式 cipher.init(Cipher.ENCRYPT_MODE, key); //执行操作 return cipher.doFinal(data); } /** * 解密 * * @param data 待解密数据 * @param key 二进制密钥 * @return byte[] 解密数据 * @throws Exception */ public static byte[] decrypt(byte[] data, byte[] key) throws Exception { return decrypt(data, key, DEFAULT_CIPHER_ALGORITHM); } /** * 解密 * * @param data 待解密数据 * @param key 密钥 * @return byte[] 解密数据 * @throws Exception */ public static byte[] decrypt(byte[] data, Key key) throws Exception { return decrypt(data, key, DEFAULT_CIPHER_ALGORITHM); } /** * 解密 * * @param data 待解密数据 * @param key 二进制密钥 * @param cipherAlgorithm 加密算法/工作模式/填充方式 * @return byte[] 解密数据 * @throws Exception */ public static byte[] decrypt(byte[] data, byte[] key, String cipherAlgorithm) throws Exception { //还原密钥 Key k = toKey(key); return decrypt(data, k, cipherAlgorithm); } /** * 解密 * * @param data 待解密数据 * @param key 密钥 * @param cipherAlgorithm 加密算法/工作模式/填充方式 * @return byte[] 解密数据 * @throws Exception */ public static byte[] decrypt(byte[] data, Key key, String cipherAlgorithm) throws Exception { //实例化 Cipher cipher = Cipher.getInstance(cipherAlgorithm); //使用密钥初始化，设置为解密模式 cipher.init(Cipher.DECRYPT_MODE, key); //执行操作 return cipher.doFinal(data); } private static String showByteArray(byte[] data) { if (null == data) { return null; } StringBuilder sb = new StringBuilder(\"{\"); for (byte b : data) { sb.append(b).append(\",\"); } sb.deleteCharAt(sb.length() - 1); sb.append(\"}\"); return sb.toString(); } /** * 使用示例 * * @param args * @throws Exception */ public static void main(String[] args) throws Exception { String password = AES_KEY; byte[] key = getSecretKey(password); System.out.println(\"key：\" + showByteArray(key)); Key k = toKey(key); String data = \"AES数据\"; System.out.println(\"加密前数据: string:\" + data); System.out.println(\"加密前数据: byte[]:\" + showByteArray(data.getBytes())); System.out.println(); byte[] encryptData = encrypt(data.getBytes(), k); String tmp = Hex.encodeHexString(encryptData); System.out.println(\"加密后数据: byte[]:\" + showByteArray(encryptData)); System.out.println(\"加密后数据: hexStr:\" + tmp); System.out.println(\"加密后数据: hexStr:\" + showByteArray(Hex.decodeHex(tmp.toCharArray()))); System.out.println(\"加密后数据: hexStr:\" + Hex.encodeHexString(Hex.decodeHex(tmp.toCharArray()))); System.out.println(); byte[] decryptData = decrypt(Hex.decodeHex(tmp.toCharArray()), k); System.out.println(\"解密后数据: byte[]:\" + showByteArray(decryptData)); System.out.println(\"解密后数据: string:\" + new String(decryptData)); }} 实现RSA加解密（非对称加密算法）该算法可以实现应用license，有以下几个步骤： 甲方构建密钥对（公钥和私钥，公钥给对方，私钥留给自己） 甲方使用私钥加密数据，然后用私钥对加密后的数据签名，并把这些发送给乙方；乙方使用公钥、签名来验证待解密数据是否有效，如果有效使用公钥对数据解密。 乙方使用公钥加密数据，向甲方发送经过加密后的数据；甲方获得加密数据，通过私钥解密。 注意：RSA加密明文最大长度117字节，解密要求密文最大长度为128字节，所以在加密和解密的过程中需要分块进行 注意：指定加密密码生成的秘钥都是一致的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346package com.systex.bh.utils;import java.io.ByteArrayOutputStream;import java.security.Key;import java.security.KeyFactory;import java.security.KeyPair;import java.security.KeyPairGenerator;import java.security.PrivateKey;import java.security.PublicKey;import java.security.SecureRandom;import java.security.Signature;import java.security.interfaces.RSAPrivateKey;import java.security.interfaces.RSAPublicKey;import java.security.spec.PKCS8EncodedKeySpec;import java.security.spec.X509EncodedKeySpec;import java.util.HashMap;import java.util.Map;import javax.crypto.Cipher;import org.apache.commons.codec.binary.Hex;public class RSAUtil { //加密算法RSA public static final String KEY_ALGORITHM = \"RSA\"; //签名算法 public static final String SIGNATURE_ALGORITHM = \"MD5withRSA\"; //获取公钥的key private static final String PUBLIC_KEY = \"RSAPublicKey\"; //获取私钥的key private static final String PRIVATE_KEY = \"RSAPrivateKey\"; //RSA最大加密明文大小 private static final int MAX_ENCRYPT_BLOCK = 117; //RSA最大解密密文大小 private static final int MAX_DECRYPT_BLOCK = 128; /** * &lt;p&gt; * 按照指定字符串生成密钥对(公钥和私钥) * &lt;/p&gt; * @param randomInfo 加密时的密码，可以指定 * @return * @throws Exception */ public static Map&lt;String, Object&gt; genKeyPair(String randomInfo) throws Exception { KeyPairGenerator keyPairGen = KeyPairGenerator.getInstance(KEY_ALGORITHM); if(null == randomInfo || \"\".equals(randomInfo)){ //不指定字符串生成密钥对 keyPairGen.initialize(1024); } else { //按照指定字符串生成密钥对 SecureRandom random = SecureRandom.getInstance(\"SHA1PRNG\"); random.setSeed(randomInfo.getBytes()); keyPairGen.initialize(1024, random); } KeyPair keyPair = keyPairGen.generateKeyPair(); RSAPublicKey publicKey = (RSAPublicKey) keyPair.getPublic(); RSAPrivateKey privateKey = (RSAPrivateKey) keyPair.getPrivate(); Map&lt;String, Object&gt; keyMap = new HashMap&lt;String, Object&gt;(2); keyMap.put(PUBLIC_KEY, publicKey); keyMap.put(PRIVATE_KEY, privateKey); return keyMap; } /** * &lt;p&gt; * 不指定字符串生成密钥对(公钥和私钥) * &lt;/p&gt; */ public static Map&lt;String, Object&gt; genKeyPair() throws Exception { return genKeyPair(null); } /** * &lt;p&gt; * 用私钥对信息生成数字签名 * &lt;/p&gt; * * @param data 已加密数据 * @param privateKey 私钥(BASE64编码) * * @return * @throws Exception */ public static String sign(byte[] data, String privateKey) throws Exception { byte[] keyBytes = Base64Util.decode(privateKey); PKCS8EncodedKeySpec pkcs8KeySpec = new PKCS8EncodedKeySpec(keyBytes); KeyFactory keyFactory = KeyFactory.getInstance(KEY_ALGORITHM); PrivateKey privateK = keyFactory.generatePrivate(pkcs8KeySpec); Signature signature = Signature.getInstance(SIGNATURE_ALGORITHM); signature.initSign(privateK); signature.update(data); return Base64Util.encode(signature.sign()); } /** * &lt;p&gt; * 校验数字签名 * &lt;/p&gt; * * @param data 已加密数据 * @param publicKey 公钥(BASE64编码) * @param sign 数字签名 * * @return * @throws Exception * */ public static boolean verify(byte[] data, String publicKey, String sign) throws Exception { byte[] keyBytes = Base64Util.decode(publicKey); X509EncodedKeySpec keySpec = new X509EncodedKeySpec(keyBytes); KeyFactory keyFactory = KeyFactory.getInstance(KEY_ALGORITHM); PublicKey publicK = keyFactory.generatePublic(keySpec); Signature signature = Signature.getInstance(SIGNATURE_ALGORITHM); signature.initVerify(publicK); signature.update(data); return signature.verify(Base64Util.decode(sign)); } /** * &lt;P&gt; * 私钥解密 * &lt;/p&gt; * * @param encryptedData 已加密数据 * @param privateKey 私钥(BASE64编码) * @return * @throws Exception */ public static byte[] decryptByPrivateKey(byte[] encryptedData, String privateKey) throws Exception { byte[] keyBytes = Base64Util.decode(privateKey); PKCS8EncodedKeySpec pkcs8KeySpec = new PKCS8EncodedKeySpec(keyBytes); KeyFactory keyFactory = KeyFactory.getInstance(KEY_ALGORITHM); Key privateK = keyFactory.generatePrivate(pkcs8KeySpec); Cipher cipher = Cipher.getInstance(keyFactory.getAlgorithm()); cipher.init(Cipher.DECRYPT_MODE, privateK); int inputLen = encryptedData.length; ByteArrayOutputStream out = new ByteArrayOutputStream(); int offSet = 0; byte[] cache; int i = 0; // 对数据分段解密 while (inputLen - offSet &gt; 0) { if (inputLen - offSet &gt; MAX_DECRYPT_BLOCK) { cache = cipher.doFinal(encryptedData, offSet, MAX_DECRYPT_BLOCK); } else { cache = cipher.doFinal(encryptedData, offSet, inputLen - offSet); } out.write(cache, 0, cache.length); i++; offSet = i * MAX_DECRYPT_BLOCK; } byte[] decryptedData = out.toByteArray(); out.close(); return decryptedData; } /** * &lt;p&gt; * 公钥解密 * &lt;/p&gt; * * @param encryptedData 已加密数据 * @param publicKey 公钥(BASE64编码) * @return * @throws Exception */ public static byte[] decryptByPublicKey(byte[] encryptedData, String publicKey) throws Exception { byte[] keyBytes = Base64Util.decode(publicKey); X509EncodedKeySpec x509KeySpec = new X509EncodedKeySpec(keyBytes); KeyFactory keyFactory = KeyFactory.getInstance(KEY_ALGORITHM); Key publicK = keyFactory.generatePublic(x509KeySpec); Cipher cipher = Cipher.getInstance(keyFactory.getAlgorithm()); cipher.init(Cipher.DECRYPT_MODE, publicK); int inputLen = encryptedData.length; ByteArrayOutputStream out = new ByteArrayOutputStream(); int offSet = 0; byte[] cache; int i = 0; // 对数据分段解密 while (inputLen - offSet &gt; 0) { if (inputLen - offSet &gt; MAX_DECRYPT_BLOCK) { cache = cipher.doFinal(encryptedData, offSet, MAX_DECRYPT_BLOCK); } else { cache = cipher.doFinal(encryptedData, offSet, inputLen - offSet); } out.write(cache, 0, cache.length); i++; offSet = i * MAX_DECRYPT_BLOCK; } byte[] decryptedData = out.toByteArray(); out.close(); return decryptedData; } /** * &lt;p&gt; * 公钥加密 * &lt;/p&gt; * * @param data 源数据 * @param publicKey 公钥(BASE64编码) * @return * @throws Exception */ public static byte[] encryptByPublicKey(byte[] data, String publicKey) throws Exception { byte[] keyBytes = Base64Util.decode(publicKey); X509EncodedKeySpec x509KeySpec = new X509EncodedKeySpec(keyBytes); KeyFactory keyFactory = KeyFactory.getInstance(KEY_ALGORITHM); Key publicK = keyFactory.generatePublic(x509KeySpec); // 对数据加密 Cipher cipher = Cipher.getInstance(keyFactory.getAlgorithm()); cipher.init(Cipher.ENCRYPT_MODE, publicK); int inputLen = data.length; ByteArrayOutputStream out = new ByteArrayOutputStream(); int offSet = 0; byte[] cache; int i = 0; // 对数据分段加密 while (inputLen - offSet &gt; 0) { if (inputLen - offSet &gt; MAX_ENCRYPT_BLOCK) { cache = cipher.doFinal(data, offSet, MAX_ENCRYPT_BLOCK); } else { cache = cipher.doFinal(data, offSet, inputLen - offSet); } out.write(cache, 0, cache.length); i++; offSet = i * MAX_ENCRYPT_BLOCK; } byte[] encryptedData = out.toByteArray(); out.close(); return encryptedData; } /** * &lt;p&gt; * 私钥加密 * &lt;/p&gt; * * @param data 源数据 * @param privateKey 私钥(BASE64编码) * @return * @throws Exception */ public static byte[] encryptByPrivateKey(byte[] data, String privateKey) throws Exception { byte[] keyBytes = Base64Util.decode(privateKey); PKCS8EncodedKeySpec pkcs8KeySpec = new PKCS8EncodedKeySpec(keyBytes); KeyFactory keyFactory = KeyFactory.getInstance(KEY_ALGORITHM); Key privateK = keyFactory.generatePrivate(pkcs8KeySpec); Cipher cipher = Cipher.getInstance(keyFactory.getAlgorithm()); cipher.init(Cipher.ENCRYPT_MODE, privateK); int inputLen = data.length; ByteArrayOutputStream out = new ByteArrayOutputStream(); int offSet = 0; byte[] cache; int i = 0; // 对数据分段加密 while (inputLen - offSet &gt; 0) { if (inputLen - offSet &gt; MAX_ENCRYPT_BLOCK) { cache = cipher.doFinal(data, offSet, MAX_ENCRYPT_BLOCK); } else { cache = cipher.doFinal(data, offSet, inputLen - offSet); } out.write(cache, 0, cache.length); i++; offSet = i * MAX_ENCRYPT_BLOCK; } byte[] encryptedData = out.toByteArray(); out.close(); return encryptedData; } /** * &lt;p&gt; * 获取私钥 * &lt;/p&gt; * * @param keyMap 密钥对 * @return * @throws Exception */ public static String getPrivateKey(Map&lt;String, Object&gt; keyMap) throws Exception { Key key = (Key) keyMap.get(PRIVATE_KEY); return Base64Util.encode(key.getEncoded()); } /** * &lt;p&gt; * 获取公钥 * &lt;/p&gt; * * @param keyMap 密钥对 * @return * @throws Exception */ public static String getPublicKey(Map&lt;String, Object&gt; keyMap) throws Exception { Key key = (Key) keyMap.get(PUBLIC_KEY); return Base64Util.encode(key.getEncoded()); } /** * 使用示例 * @param args * @throws Exception */ public static void main(String[] args) throws Exception { String publicKey = null; String privateKey = null; String password = \"SYSTEX\"; Map&lt;String, Object&gt; keyMap = RSAUtil.genKeyPair(password); publicKey = RSAUtil.getPublicKey(keyMap); privateKey = RSAUtil.getPrivateKey(keyMap); System.err.println(\"公钥: \\n\\r\" + publicKey); System.err.println(\"私钥： \\n\\r\" + privateKey); System.err.println(\"公钥加密——私钥解密\"); String source = \"这是一行没有任何意义的文字，你看完了等于没看，不是吗？\"; System.out.println(\"\\r加密前文字：\\r\\n\" + source); byte[] data = source.getBytes(\"utf-8\"); byte[] encodedData = RSAUtil.encryptByPublicKey(data, publicKey); System.out.println(\"加密后文字：\\r\\n\" + Hex.encodeHexString(encodedData)); byte[] decodedData = RSAUtil.decryptByPrivateKey(encodedData, privateKey); String target = new String(decodedData); System.out.println(\"解密后文字: \\r\\n\" + target); System.err.println(\"私钥加密——公钥解密\"); String source2 = \"这是一行测试RSA数字签名的无意义文字\"; System.out.println(\"原文字：\\r\\n\" + source2); byte[] data2 = source2.getBytes(); byte[] encodedData2 = RSAUtil.encryptByPrivateKey(data2, privateKey); System.out.println(\"加密后：\\r\\n\" + Hex.encodeHexString(encodedData2)); byte[] decodedData2 = RSAUtil.decryptByPublicKey(encodedData2, publicKey); String target2 = new String(decodedData2); System.out.println(\"解密后: \\r\\n\" + target2); System.err.println(\"私钥签名——公钥验证签名\"); String sign = RSAUtil.sign(encodedData2, privateKey); System.err.println(\"签名:\\r\" + sign); boolean status = RSAUtil.verify(encodedData2, publicKey, sign); System.err.println(\"验证结果:\\r\" + status); }}","link":"/2019/12/28/java-AES加解密和RSA加解密/"},{"title":"spark-shuffle性能优化","text":"Shuffle操作大概是对Spark性能影响最大的步骤之一（因为可能涉及到排序，磁盘IO，网络IO等众多CPU或IO密集的操作），这也是为什么在Spark 1.1的代码中对整个Shuffle框架代码进行了重构，将Shuffle相关读写操作抽象封装到Pluggable的Shuffle Manager中，便于试验和实现不同的Shuffle功能模块。例如为了解决Hash Based的Shuffle Manager在文件读写效率方面的问题而实现的Sort Base的Shuffle Manager。 shuffle操作有map和reduce阶段： map阶段：计算出数据的key值，并计算key值对应的hash值 reduce阶段：根据hash值进行reduce操作，将各个Reduce分区（就是根据hash值相同的作为一个分区，类似分组）的数据写到各自的磁盘文件中 shuffle性能优化参数设置(1) spark.shuffle.manager用来配置所使用的Shuffle Manager，目前可选的Shuffle Manager包括默认的以下两个： org.apache.spark.shuffle.sort.HashShuffleManager（配置参数值为hash） org.apache.spark.shuffle.sort.SortShuffleManager（配置参数值为sort）这两个ShuffleManager如何选择呢，首先需要了解他们在实现方式上的区别。 HashShuffleManager，故名思义也就是在Shuffle的过程中写数据时不做排序操作，只是将数据根据Hash的结果，将各个Reduce分区的数据写到各自的磁盘文件中。带来的问题就是如果Reduce分区的数量比较大的话，将会产生大量的磁盘文件。如果文件数量特别巨大，对文件读写的性能会带来比较大的影响，此外由于同时打开的文件句柄数量众多，序列化，以及压缩等操作需要分配的临时内存空间也可能会迅速膨胀到无法接受的地步，对内存的使用和GC带来很大的压力，在Executor内存比较小的情况下尤为突出，例如Spark on Yarn模式。 SortShuffleManager，是1.1版本之后实现的一个试验性（也就是一些功能和接口还在开发演变中）的ShuffleManager，它在写入分区数据的时候，首先会根据实际情况对数据采用不同的方式进行排序操作，底线是至少按照Reduce分区Partition进行排序，这样来至于同一个Map任务Shuffle到不同的Reduce分区中去的所有数据都可以写入到同一个外部磁盘文件中去，用简单的Offset标志不同Reduce分区的数据在这个文件中的偏移量。这样一个Map任务就只需要生成一个shuffle文件，从而避免了上述HashShuffleManager可能遇到的文件数量巨大的问题 两者的性能比较，取决于内存，排序，文件操作等因素的综合影响。 对于不需要进行排序的Shuffle操作来说，如repartition等，如果文件数量不是特别巨大，HashShuffleManager面临的内存问题不大，而SortShuffleManager需要额外的根据Partition进行排序，显然HashShuffleManager的效率会更高。 而对于本来就需要在Map端进行排序的Shuffle操作来说，如ReduceByKey等，使用HashShuffleManager虽然在写数据时不排序，但在其它的步骤中仍然需要排序，而SortShuffleManager则可以将写数据和排序两个工作合并在一起执行，因此即使不考虑HashShuffleManager的内存使用问题，SortShuffleManager依旧可能更快。 (2) spark.shuffle.sort.bypassMergeThreshold这个参数仅适用于SortShuffleManager，如前所述，SortShuffleManager在处理不需要排序的Shuffle操作时，由于排序带来性能的下降。这个参数决定了在这种情况下，当Reduce分区的数量小于多少的时候，在SortShuffleManager内部不使用Merge Sort的方式处理数据，而是与Hash Shuffle类似，直接将分区文件写入单独的文件，不同的是，在最后一步还是会将这些文件合并成一个单独的文件。这样通过去除Sort步骤来加快处理速度，代价是需要并发打开多个文件，所以内存消耗量增加，本质上是相对HashShuffleMananger一个折衷方案。 这个参数的默认值是200个分区，如果内存GC问题严重，可以降低这个值。 (3) spark.shuffle.consolidateFiles这个配置参数仅适用于HashShuffleMananger的实现，同样是为了解决生成过多文件的问题，采用的方式是在不同批次运行的Map任务之间重用Shuffle输出文件，也就是说合并的是不同批次的Map任务的输出数据，但是每个Map任务所需要的文件还是取决于Reduce分区的数量，因此，它并不减少同时打开的输出文件的数量，因此对内存使用量的减少并没有帮助。只是HashShuffleManager里的一个折中的解决方案。 需要注意的是，这部分的代码实现尽管原理上说很简单，但是涉及到底层具体的文件系统的实现和限制等因素，例如在并发访问等方面，需要处理的细节很多，因此一直存在着这样那样的bug或者问题，导致在例如EXT3上使用时，特定情况下性能反而可能下降，因此从Spark 0.8的代码开始，一直到Spark 1.1的代码为止也还没有被标志为Stable，不是默认采用的方式。此外因为并不减少同时打开的输出文件的数量，因此对性能具体能带来多大的改善也取决于具体的文件数量的情况。所以即使你面临着Shuffle文件数量巨大的问题，这个配置参数是否使用，在什么版本中可以使用，也最好还是实际测试以后再决定。 (4) spark.shuffle.spillshuffle的过程中，如果涉及到排序，聚合等操作，势必会需要在内存中维护一些数据结构，进而占用额外的内存。如果内存不够用怎么办，那只有两条路可以走，一就是out of memory 出错了，二就是将部分数据临时写到外部存储设备中去，最后再合并到最终的Shuffle输出文件中去。 这里spark.shuffle.spill 决定是否Spill到外部存储设备（默认打开）,如果你的内存足够使用，或者数据集足够小，当然也就不需要Spill，毕竟Spill带来了额外的磁盘操作。 (5) spark.shuffle.memoryFraction / spark.shuffle.safetyFraction在启用Spill的情况下，spark.shuffle.memoryFraction（1.1后默认为0.2）决定了当Shuffle过程中使用的内存达到总内存多少比例的时候开始Spill。 通过spark.shuffle.memoryFraction可以调整Spill的触发条件，即Shuffle占用内存的大小，进而调整Spill的频率和GC的行为。总的来说，如果Spill太过频繁，可以适当增加spark.shuffle.memoryFraction的大小，增加用于Shuffle的内存，减少Spill的次数。当然这样一来为了避免内存溢出，对应的可能需要减少RDD cache占用的内存，即减小spark.storage.memoryFraction的值，这样RDD cache的容量减少，有可能带来性能影响，因此需要综合考虑。 由于Shuffle数据的大小是估算出来的，一来为了降低开销，并不是每增加一个数据项都完整的估算一次，二来估算也会有误差，所以实际暂用的内存可能比估算值要大，这里spark.shuffle.safetyFraction（默认为0.8）用来作为一个保险系数，降低实际Shuffle使用的内存阀值，增加一定的缓冲，降低实际内存占用超过用户配置值的概率。 (6) spark.shuffle.spill.compress / spark.shuffle.compress这两个配置参数都是用来设置Shuffle过程中是否使用压缩算法对Shuffle数据进行压缩，前者针对Spill的中间数据，后者针对最终的shuffle输出文件，默认都是True 理论上说，spark.shuffle.compress设置为True通常都是合理的，因为如果使用千兆以下的网卡，网络带宽往往最容易成为瓶颈。此外，目前的Spark任务调度实现中，以Shuffle划分Stage，下一个Stage的任务是要等待上一个Stage的任务全部完成以后才能开始执行，所以shuffle数据的传输和CPU计算任务之间通常不会重叠，这样Shuffle数据传输量的大小和所需的时间就直接影响到了整个任务的完成速度。但是压缩也是要消耗大量的CPU资源的，所以打开压缩选项会增加Map任务的执行时间，因此如果在CPU负载的影响远大于磁盘和网络带宽的影响的场合下，也可能将spark.shuffle.compress 设置为False才是最佳的方案 对于spark.shuffle.spill.compress而言，情况类似，但是spill数据不会被发送到网络中，仅仅是临时写入本地磁盘，而且在一个任务中同时需要执行压缩和解压缩两个步骤，所以对CPU负载的影响会更大一些，而磁盘带宽（如果标配12HDD的话）可能往往不会成为Spark应用的主要问题，所以这个参数相对而言，或许更有机会需要设置为False。 总之，Shuffle过程中数据是否应该压缩，取决于CPU/DISK/NETWORK的实际能力和负载，应该综合考虑。","link":"/2016/07/31/spark-shuffle-optimization/"},{"title":"spark-压缩和序列化相关","text":"压缩和序列化性能优化参数设置(1) spark.serializer默认为org.apache.spark.serializer.JavaSerializer, 可选 org.apache.spark.serializer.KryoSerializer, 实际上只要是org.apache.spark.serializer的子类就可以了,不过如果只是应用,大概你不会自己去实现一个的。 序列化对于spark应用的性能来说,还是有很大影响的,在特定的数据格式的情况下,KryoSerializer的性能可以达到JavaSerializer的10倍以上,当然放到整个Spark程序中来考量,比重就没有那么大了,但是以Wordcount为例，通常也很容易达到30%以上的性能提升。而对于一些Int之类的基本类型数据，性能的提升就几乎可以忽略了。KryoSerializer依赖Twitter的Chill库来实现，相对于JavaSerializer，主要的问题在于不是所有的Java Serializable对象都能支持。 需要注意的是，这里可配的Serializer针对的对象是Shuffle数据，以及RDD Cache等场合，而Spark Task的序列化是通过spark.closure.serializer来配置，但是目前只支持JavaSerializer，所以等于没法配置啦。 (2) spark.rdd.compress这个参数决定了RDD Cache的过程中，RDD数据在序列化之后是否进一步进行压缩再储存到内存或磁盘上。当然是为了进一步减小Cache数据的尺寸，对于Cache在磁盘上而言，绝对大小大概没有太大关系，主要是考虑Disk的IO带宽。而对于Cache在内存中，那主要就是考虑尺寸的影响，是否能够Cache更多的数据，是否能减小Cache数据对GC造成的压力等。 这两者，前者通常不会是主要问题，尤其是在RDD Cache本身的目的就是追求速度，减少重算步骤，用IO换CPU的情况下。而后者，GC问题当然是需要考量的，数据量小，占用空间少，GC的问题大概会减轻，但是是否真的需要走到RDD Cache压缩这一步，或许用其它方式来解决可能更加有效。 所以这个值默认是关闭的，但是如果在磁盘IO的确成为问题或者GC问题真的没有其它更好的解决办法的时候，可以考虑启用RDD压缩。 (3) spark.broadcast.compress是否对Broadcast的数据进行压缩，默认值为True。 Broadcast机制是用来减少运行每个Task时，所需要发送给TASK的RDD所使用到的相关数据的尺寸，一个Executor只需要在第一个Task启动时，获得一份Broadcast数据，之后的Task都从本地的BlockManager中获取相关数据。在1.1最新版本的代码中，RDD本身也改为以Broadcast的形式发送给Executor（之前的实现RDD本身是随每个任务发送的），因此基本上不太需要显式的决定哪些数据需要broadcast了。 因为Broadcast的数据需要通过网络发送，而在Executor端又需要存储在本地BlockMananger中，加上最新的实现，默认RDD通过Boradcast机制发送，因此大大增加了Broadcast变量的比重，所以通过压缩减小尺寸，来减少网络传输开销和内存占用，通常都是有利于提高整体性能的。 什么情况可能不压缩更好呢，大致上个人觉得同样还是在网络带宽和内存不是问题的时候，如果Driver端CPU资源很成问题（毕竟压缩的动作基本都在Driver端执行），那或许有调整的必要。 (4) spark.io.compression.codecRDD Cache和Shuffle数据压缩所采用的算法Codec，默认值曾经是使用LZF作为默认Codec，最近因为LZF的内存开销的问题，默认的Codec已经改为Snappy。 LZF和Snappy相比较，前者压缩率比较高（当然要看具体数据内容了，通常要高20%左右），但是除了内存问题以外，CPU代价也大一些（大概也差20%~50%？） 在用于Shuffle数据的场合下，内存方面，应该主要是在使用HashShuffleManager的时候有可能成为问题，因为如果Reduce分区数量巨大，需要同时打开大量的压缩数据流用于写文件，进而在Codec方面需要大量的buffer。但是如果使用SortShuffleManager，由于shuffle文件数量大大减少，不会产生大量的压缩数据流，所以内存开销大概不会成为主要问题。 剩下的就是CPU和压缩率的权衡取舍，和前面一样，取决于CPU/网络/磁盘的能力和负载，个人认为CPU通常更容易成为瓶颈。所以要调整性能，要不不压缩，要不使用Snappy可能性大一些？ 对于RDD Cache的场合来说，绝大多数场合都是内存操作或者本地IO，所以CPU负载的问题可能比IO的问题更加突出，这也是为什么 spark.rdd.compress 本身默认为不压缩，如果要压缩，大概也是Snappy合适一些？","link":"/2019/12/28/spark-压缩和序列化相关/"},{"title":"spark-storage性能优化","text":"storage性能优化参数设置(1) spark.local.dir这个看起来很简单，就是Spark用于写中间数据，如RDD Cache，Shuffle，Spill等数据的位置，那么有什么可以注意的呢。 首先，最基本的当然是我们可以配置多个路径（用逗号分隔）到多个磁盘上增加整体IO带宽，这个大家都知道。 其次，目前的实现中，Spark是通过对文件名采用hash算法分布到多个路径下的目录中去，如果你的存储设备有快有慢，比如SSD+HDD混合使用，那么你可以通过在SSD上配置更多的目录路径来增大它被Spark使用的比例，从而更好地利用SSD的IO带宽能力。当然这只是一种变通的方法，终极解决方案还是应该像目前HDFS的实现方向一样，让Spark能够感知具体的存储设备类型，针对性的使用。 需要注意的是，在Spark 1.0 以后，SPARK_LOCAL_DIRS (Standalone, Mesos) or LOCAL_DIRS (YARN)参数会覆盖这个配置。比如Spark On YARN的时候，Spark Executor的本地路径依赖于Yarn的配置，而不取决于这个参数。 (2) spark.executor.memoryExecutor 内存的大小，和性能本身当然并没有直接的关系，但是几乎所有运行时性能相关的内容都或多或少间接和内存大小相关。这个参数最终会被设置到Executor的JVM的heap尺寸上，对应的就是Xmx和Xms的值 理论上Executor 内存当然是多多益善，但是实际受机器配置，以及运行环境，资源共享，JVM GC效率等因素的影响，还是有可能需要为它设置一个合理的大小。 多大算合理，要看实际情况 Executor的内存基本上是Executor内部所有任务共享的，而每个Executor上可以支持的任务的数量取决于Executor所管理的CPU Core资源的多少，因此你需要了解每个任务的数据规模的大小，从而推算出每个Executor大致需要多少内存即可满足基本的需求。 如何知道每个任务所需内存的大小呢，这个很难统一的衡量，因为除了数据集本身的开销，还包括算法所需各种临时内存空间的使用，而根据具体的代码算法等不同，临时内存空间的开销也不同。但是数据集本身的大小，对最终所需内存的大小还是有一定的参考意义的。 通常来说每个分区的数据集在内存中的大小，可能是其在磁盘上源数据大小的若干倍（不考虑源数据压缩，Java对象相对于原始裸数据也还要算上用于管理数据的数据结构的额外开销），需要准确的知道大小的话，可以将RDD cache在内存中，从BlockManager的Log输出可以看到每个Cache分区的大小（其实也是估算出来的，并不完全准确） 如： BlockManagerInfo: Added rdd_0_1 on disk on sr438:41134 (size: 495.3 MB)反过来说，如果你的Executor的数量和内存大小受机器物理配置影响相对固定，那么你就需要合理规划每个分区任务的数据规模，例如采用更多的分区，用增加任务数量（进而需要更多的批次来运算所有的任务）的方式来减小每个任务所需处理的数据大小。 (3) spark.storage.memoryFraction如前面所说spark.executor.memory决定了每个Executor可用内存的大小，而spark.storage.memoryFraction则决定了在这部分内存中有多少可以用于Memory Store管理RDD Cache数据，剩下的内存用来保证任务运行时各种其它内存空间的需要。 spark.executor.memory默认值为0.6，官方文档建议这个比值不要超过JVM Old Gen区域的比值。这也很容易理解，因为RDD Cache数据通常都是长期驻留内存的，理论上也就是说最终会被转移到Old Gen区域（如果该RDD还没有被删除的话），如果这部分数据允许的尺寸太大，势必把Old Gen区域占满，造成频繁的FULL GC。 如何调整这个比值，取决于你的应用对数据的使用模式和数据的规模，粗略的来说，如果频繁发生Full GC，可以考虑降低这个比值，这样RDD Cache可用的内存空间减少（剩下的部分Cache数据就需要通过Disk Store写到磁盘上了），会带来一定的性能损失，但是腾出更多的内存空间用于执行任务，减少Full GC发生的次数，反而可能改善程序运行的整体性能 (4) spark.streaming.blockInterval这个参数用来设置Spark Streaming里Stream Receiver生成Block的时间间隔，默认为200ms。具体的行为表现是具体的Receiver所接收的数据，每隔这里设定的时间间隔，就从Buffer中生成一个StreamBlock放进队列，等待进一步被存储到BlockManager中供后续计算过程使用。理论上来说，为了每个Streaming Batch 间隔里的数据是均匀的，这个时间间隔当然应该能被Batch的间隔时间长度所整除。总体来说，如果内存大小够用，Streaming的数据来得及处理，这个blockInterval时间间隔的影响不大，当然，如果数据Cache Level是Memory+Ser，即做了序列化处理，那么BlockInterval的大小会影响序列化后数据块的大小，对于Java 的GC的行为会有一些影响。 此外spark.streaming.blockQueueSize决定了在StreamBlock被存储到BlockMananger之前，队列中最多可以容纳多少个StreamBlock。默认为10，因为这个队列Poll的时间间隔是100ms，所以如果CPU不是特别繁忙的话，基本上应该没有问题。","link":"/2019/12/28/spark-storage-optimization/"},{"title":"spark-updateStateByKey状态更新","text":"SparkStreaming是基于时间分片的近实时处理组件，比如需要实时对日志进行词频统计，通常是使用reduceByKey来处理，这样处理的结果是每一个时间片段的词频统计，例如第n个时间片的处理结果是[hello:3, world:5]，第n+1个时间片的处理结果是[hello:2, world:2]，但是这并不是我们期望的结果，我们期望得到的结果是不断累积各个时间片的词频，如上面的例子所希望的结果应该是[hello:5, world:7]。 spark提供一个状态更新的方法updateStateByKey，使用场景：第n个时间片处理是基于第n-1个时间片的结果。这里举一个复杂一点的例子：统计用户登录失败次数，超过阀值则告警，产生告警之后的5分钟内，不再重复发送该告警。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145package com.demo.hl;import com.google.common.base.Optional;import kafka.serializer.StringDecoder;import org.apache.spark.SparkConf;import org.apache.spark.streaming.Duration;import org.apache.spark.streaming.api.java.JavaPairInputDStream;import org.apache.spark.streaming.api.java.JavaStreamingContext;import org.apache.spark.streaming.kafka.KafkaUtils;import scala.Tuple2;import java.io.Serializable;import java.util.*;import java.util.regex.Matcher;import java.util.regex.Pattern;/** * 窗口操作 * * 统计用户登录失败超过阀值告警，产生告警之后的5分钟内，不再重复发送该告警 * 这了使用到了一个状态更新的api : updateStateByKey */public class WindowEventTriggerDemo{ public static void main(String[] args) { String login_regs = \"(\\\\d+-\\\\d+-\\\\d+\\\\s\\\\d+:\\\\d+:\\\\d+)\\\\s(\\\\w+)\\\\s.*\"; Pattern pattern = Pattern.compile(login_regs); int triggerLevel = 10; int windowTimeLength = 1; //单位：分钟 int slideLength = 10; //单位：秒 int skipAlarmBatchDuration = (windowTimeLength * 60) / slideLength; Duration windowDuration = new Duration(windowTimeLength * 60 * 1000); Duration slideDuration = new Duration(slideLength * 1000); Set&lt;String&gt; loginTopicsSet = new HashSet&lt;&gt;(Arrays.asList(\"rest_topic\".split(\",\"))); Map&lt;String, String&gt; kafkaParams = new HashMap&lt;&gt;(); kafkaParams.put(\"metadata.broker.list\", \"master:9092,slave1:9092\"); kafkaParams.put(\"group.id\", \"consummer1\"); kafkaParams.put(\"serializer.class\", \"kafka.serializer.StringEncoder\"); SparkConf sparkConf = new SparkConf().setAppName(\"WindowEventTriggerCard\").setMaster(\"local[4]\"); JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, new Duration(10 * 1000)); JavaPairInputDStream&lt;String, String&gt; loginDstream = KafkaUtils .createDirectStream(jssc, String.class, String.class, StringDecoder.class, StringDecoder.class, kafkaParams, loginTopicsSet); jssc.checkpoint(\"hdfs://slave1:9000/user/checkpoints\"); //过滤登录失败的记录 loginDstream.filter(log -&gt; log._2.contains(\"login failed\")) //拆分key-value对 [key : user, value : windowEventTrigger] .mapToPair(log -&gt; { Matcher matcher = pattern.matcher(log._2); String user = null; String time = null; if (matcher.find()) { user = matcher.group(2); time = matcher.group(1); } else { user = \"NULL-user\"; time = \"NULL-time\"; } WindowEventTrigger windowEventTrigger = new WindowEventTrigger(); windowEventTrigger.eventsInWindow.add(time); //key : user , value : timeList return new Tuple2&lt;&gt;(user, windowEventTrigger); }) //5分钟增量窗口 .reduceByKeyAndWindow(WindowEventTrigger::add, WindowEventTrigger::remove, windowDuration, slideDuration) //更新状态 .updateStateByKey((List&lt;WindowEventTrigger&gt; newEvents, Optional&lt;WindowEventTrigger&gt; state) -&gt; { //获取前一个批次保存的状态信息，并把最新的数据更新到状态对象里面 WindowEventTrigger stat_value = state.isPresent() ? state.get() : new WindowEventTrigger(); stat_value.eventsInWindow.clear(); if (stat_value.flag == 0 &amp;&amp; !stat_value.isAlarm) { for (WindowEventTrigger e : newEvents) { stat_value.eventsInWindow.addAll(e.eventsInWindow); } } //如果登录失败次数超过阀值，且flag=0 和 isAlarm=false，则需要告警：设置isAlarm=true 且 flag+1 //如果登陆失败次数低于阀值，且flag=0 和 isAlarm=false，则不需要任何操作 //否则如果flag&lt;skipAlarmBatchDuration，则不告警，直到flag==skipAlarmBatchDuration时，把flag回复为0，也不告警，不过此时的flag和isAlarm已经回复为最初状态：0 和 false //这样做的目的是在产生告警的之后的skipAlarmBatchDuration个批次时间内不再重复发送该告警 if (stat_value.flag == 0 &amp;&amp; !stat_value.isAlarm &amp;&amp; stat_value.eventsInWindow.size() &gt;= triggerLevel) { stat_value.isAlarm = true; stat_value.flag++; } else if (stat_value.flag != 0 || stat_value.isAlarm || stat_value.eventsInWindow.size() &gt;= triggerLevel) { if (stat_value.flag &lt; skipAlarmBatchDuration) { stat_value.flag++; } else { stat_value.flag = 0; } stat_value.isAlarm = false; } return Optional.of(stat_value); }) //过滤出需要告警的记录 .filter(event -&gt; event._2.isAlarm) //发送告警信息 .foreachRDD(eventRDD -&gt; { System.out.println(\"-------------------- count ---------- &gt; \" + eventRDD.count()); eventRDD.foreachPartition(itr -&gt; { Tuple2&lt;String, WindowEventTrigger&gt; tmp = null; while (itr.hasNext()) { tmp = itr.next(); System.out.println( \"alarm --&gt; user: \" + tmp._1 + \" login failed, count: \" + tmp._2.eventsInWindow .size()); } }); return null; }); jssc.start(); jssc.awaitTermination(); }} 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * * 事件触发类 */class WindowEventTrigger implements Serializable{ private static final long serialVersionUID = 1L; public TreeSet&lt;String&gt; eventsInWindow = new TreeSet&lt;&gt;(); //告警事件列表 public boolean isAlarm = false; //是否告警 public int flag = 0; //从告警那一刻起，之后的5分钟内不再重复发送该告警，这个值不能超过 窗口长度 是 执行间隔的 倍数 public WindowEventTrigger() { } /** * add * * @param incoming * @return 把每条记录封装为事件触发器 * 注意：reduceByKeyAndWindow(_ + _, _ - _, Seconds(5s)，seconds(1))，对于这个操作的增量方法的操作，如： * 即key值一样的，把已有的 values 和新增的数据进行增量操作 * addFunction：原先总体的数据集 和 新增的数据 进行操作 * removeFunction：在addFunction操作之后的数据集 和 减掉的数据 进行操作 * &lt;p&gt; * 应用场景：同一个用户5分钟内登录失败次数超过10次产生告警，如果某用户登录产生告警之后，且之后的5分钟不再发同样的告警 */ public WindowEventTrigger add(WindowEventTrigger incoming) { //把原先触发事件集 和 新增触发事件 并集操作 this.eventsInWindow.addAll(incoming.eventsInWindow); return this; } //remove public WindowEventTrigger remove(WindowEventTrigger outgoing) { //移除values中过时的数据 this.eventsInWindow.removeAll(outgoing.eventsInWindow); return this; }}","link":"/2019/12/28/spark-updateStateByKey状态更新/"},{"title":"spark-编程常用例子（Java版）-1","text":"在实际开发中，免不了要涉及到对数据分组、实时数据和历史数据匹配、处理后的结果需要保存到其它系统（ES,Redis,Mysql等）等，这里是对这三种情况简单实现的例子。 (1) 根据多个关键字分组123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import java.util.Arrays;import java.util.HashMap;import java.util.HashSet;import java.util.Map;import java.util.Set;import java.util.regex.Matcher;import java.util.regex.Pattern;import org.apache.spark.SparkConf;import org.apache.spark.streaming.Duration;import org.apache.spark.streaming.api.java.JavaPairInputDStream;import org.apache.spark.streaming.api.java.JavaStreamingContext;import org.apache.spark.streaming.kafka.KafkaUtils;import kafka.serializer.StringDecoder;import scala.Tuple2;/** * * 根据多个字段分组 * 例子：统计每一天中同一个源IP对同一个目标IP的攻击次数 * */public class GroupDemo { public static void main(String[] args) { //2016-07-13 01:10:23 WARMING src_ip:192.1.1.100 dest_ip:10.1.1.2 attack_type:program virus infections //正则拆出：day, src_ip, dest_ip String log_regs = \"(\\\\d+-\\\\d+-\\\\d+)\\\\s\\\\d+:\\\\d+:\\\\d+\\\\s.*src_ip:(\\\\S+)\\\\sdest_ip:(\\\\S+)\\\\s.*\"; Pattern pattern = Pattern.compile(log_regs); Set&lt;String&gt; loginTopicsSet = new HashSet&lt;String&gt;(Arrays.asList(\"rest_topic\".split(\",\"))); Map&lt;String, String&gt; kafkaParams = new HashMap&lt;String, String&gt;(); kafkaParams.put(\"metadata.broker.list\", \"master:9092,slave1:9092\"); kafkaParams.put(\"group.id\", \"consummer1\"); kafkaParams.put(\"serializer.class\", \"kafka.serializer.StringEncoder\"); SparkConf sparkConf = new SparkConf().setAppName(\"WindowEventTriggerCard\").setMaster(\"local[2]\"); JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, new Duration(10 * 1000)); JavaPairInputDStream&lt;String, String&gt; logDstream = KafkaUtils.createDirectStream(jssc, String.class, String.class, StringDecoder.class, StringDecoder.class, kafkaParams, loginTopicsSet); logDstream.filter(log -&gt; log._2.contains(\"WARMING\")) .mapToPair(log -&gt; { Matcher matcher = pattern.matcher(log._2); String day = null; String src_ip = null; String dest_ip = null; if(matcher.find()) { day = matcher.group(1); src_ip = matcher.group(2); dest_ip = matcher.group(3); } else { day = \"not_matcher_user\"; src_ip = \"not_matcher_src_ip\"; dest_ip = \"not_matcher_dest_ip\"; } //把 day,src_ip,dest_ip作为key String key = day + \",\" + src_ip + \",\" + dest_ip; return new Tuple2&lt;String, String&gt;(key, log._2); }).groupByKey().print(); jssc.start(); jssc.awaitTermination(); }} (2) 实时数据和历史数据匹配注意：获取历史数据时使用分区方式获取，如mapPartitions等 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package com.demo.hl;import java.util.ArrayList;import java.util.Arrays;import java.util.HashMap;import java.util.HashSet;import java.util.List;import java.util.Map;import java.util.Set;import java.util.regex.Matcher;import java.util.regex.Pattern;import org.apache.spark.SparkConf;import org.apache.spark.streaming.Duration;import org.apache.spark.streaming.api.java.JavaPairInputDStream;import org.apache.spark.streaming.api.java.JavaStreamingContext;import org.apache.spark.streaming.kafka.KafkaUtils;import kafka.serializer.StringDecoder;import scala.Tuple2;/** * * 实时数据和历史数据匹配操作 * 建议：如果需要获取历史数据，先使用排程对历史数据预处理 * 该场景也适用实时数据和随时变化的对照档匹配处理 * 例子：实时接收用户登录数据，用户登录成功后，判断该用户前7天内是否登录且登录成功的记录，有的话属于正常，否则异常 * */public class HistoryDemo { public static void main(String[] args) { ////2016-07-13 01:10:11 admin login success //正则拆出：time, user String log_regs = \"(\\\\d+-\\\\d+-\\\\d+\\\\s\\\\d+:\\\\d+:\\\\d+)\\\\s(\\\\w+)\\\\s.*\"; Pattern pattern = Pattern.compile(log_regs); Set&lt;String&gt; loginTopicsSet = new HashSet&lt;String&gt;(Arrays.asList(\"rest_topic\".split(\",\"))); Map&lt;String, String&gt; kafkaParams = new HashMap&lt;String, String&gt;(); kafkaParams.put(\"metadata.broker.list\", \"master:9092,slave1:9092\"); kafkaParams.put(\"group.id\", \"consummer1\"); kafkaParams.put(\"serializer.class\", \"kafka.serializer.StringEncoder\"); SparkConf sparkConf = new SparkConf().setAppName(\"WindowEventTriggerCard\").setMaster(\"local[2]\"); JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, new Duration(10 * 1000)); JavaPairInputDStream&lt;String, String&gt; logDstream = KafkaUtils.createDirectStream(jssc, String.class, String.class, StringDecoder.class, StringDecoder.class, kafkaParams, loginTopicsSet); logDstream.filter(log -&gt; log._2.contains(\"login success\")) .mapToPair(log -&gt; { Matcher matcher = pattern.matcher(log._2); String user = null; user = matcher.find() ? matcher.group(2) : \"NULL-user\"; return new Tuple2&lt;String, String&gt;(user, log._2); }) .mapPartitions(log -&gt; { //排程：每天处理前一天登录成功的记录，然后保存到redis，再把最早一天的数据清除 //只需保存用户名即可，如 : channel --&gt; 2016-07-14 : Set&lt;String&gt; users //从redis获取前7天登录成功的用户 List&lt;String&gt; users = null; List&lt;Tuple2&lt;String,String&gt;&gt; list = new ArrayList&lt;Tuple2&lt;String,String&gt;&gt;(); String flag = null; Tuple2&lt;String,String&gt; tmp = null; while(log.hasNext()) { tmp = log.next(); flag = users.contains(tmp._1) ? \"nice\" : \"alert\"; list.add(new Tuple2&lt;String,String&gt;(flag, tmp._2)); } return list; }) .filter(log -&gt; log._1.equals(\"alert\")).print(); jssc.start(); jssc.awaitTermination(); }} (3) 处理后的结果需要保存到其它系统（ES,Redis,Mysql等）注意：往其它系统写数据时使用分区方式，如foreachPartition等 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798package com.demo.hl;import java.util.Arrays;import java.util.HashMap;import java.util.HashSet;import java.util.Map;import java.util.Set;import java.util.regex.Matcher;import java.util.regex.Pattern;import org.apache.spark.SparkConf;import org.apache.spark.streaming.Duration;import org.apache.spark.streaming.api.java.JavaPairInputDStream;import org.apache.spark.streaming.api.java.JavaStreamingContext;import org.apache.spark.streaming.kafka.KafkaUtils;import kafka.serializer.StringDecoder;import redis.clients.jedis.Jedis;import scala.Tuple2;/** * * 处理后的结果如果需要保存到其它系统（ES,Redis,Mysql等）,使用分区的方式 * 例子：统计每一天中同一个源IP对同一个目标IP的攻击次数，如果攻击数量超过一定阀值，则告警，数据保存到Redis * */public class WriterDemo { public static void main(String[] args) { //2016-07-13 01:10:23 WARMING src_ip:192.1.1.100 dest_ip:10.1.1.2 attack_type:program virus infections //正则拆出：day, src_ip, dest_ip, type String log_regs = \"(\\\\d+-\\\\d+-\\\\d+)\\\\s\\\\d+:\\\\d+:\\\\d+\\\\s.*src_ip:(\\\\S+)\\\\sdest_ip:(\\\\S+)\\\\sattack_type:(.*)\"; Pattern pattern = Pattern.compile(log_regs); int limit = 10; Set&lt;String&gt; loginTopicsSet = new HashSet&lt;String&gt;(Arrays.asList(\"rest_topic\".split(\",\"))); Map&lt;String, String&gt; kafkaParams = new HashMap&lt;String, String&gt;(); kafkaParams.put(\"metadata.broker.list\", \"master:9092,slave1:9092\"); kafkaParams.put(\"group.id\", \"consummer1\"); kafkaParams.put(\"serializer.class\", \"kafka.serializer.StringEncoder\"); SparkConf sparkConf = new SparkConf().setAppName(\"WindowEventTriggerCard\").setMaster(\"local[2]\"); JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, new Duration(10 * 1000)); JavaPairInputDStream&lt;String, String&gt; logDstream = KafkaUtils.createDirectStream(jssc, String.class, String.class, StringDecoder.class, StringDecoder.class, kafkaParams, loginTopicsSet); logDstream.filter(log -&gt; log._2.contains(\"WARMING\")) .mapToPair(log -&gt; { Matcher matcher = pattern.matcher(log._2); String day = null; String src_ip = null; String dest_ip = null; String type = null; if(matcher.find()) { day = matcher.group(1); src_ip = matcher.group(2); dest_ip = matcher.group(3); type = matcher.group(4); } else { day = \"not_matcher_user\"; src_ip = \"not_matcher_src_ip\"; dest_ip = \"not_matcher_dest_ip\"; type = \"ot_matcher_type\"; } //把 day,src_ip,dest_ip作为key String key = day + \",\" + src_ip + \",\" + dest_ip; Tuple2&lt;String,Integer&gt; value = new Tuple2&lt;String,Integer&gt;(type, 1); return new Tuple2&lt;String, Tuple2&lt;String,Integer&gt;&gt;(key, value); }) .reduceByKey((log1, log2) -&gt; new Tuple2&lt;String,Integer&gt;(log1._1 + \",\" + log2._1, log1._2 + log2._2)) .filter(log -&gt; log._2._2 &gt; limit) .foreachRDD(logRDD -&gt; { logRDD.foreachPartition(itr -&gt; { //获取redis客户端对象 jedis Jedis jedis = null; //往redis写数据 Tuple2&lt;String, Tuple2&lt;String, Integer&gt;&gt; tmp = null; while(itr.hasNext()) { tmp = itr.next(); jedis.hset(\"alarm.today\", tmp._1, \"alarm --&gt; 攻击次数：\" + tmp._2._2 + \"，攻击类型：\" + tmp._2._1); } //关闭redis客户端对象 jedis.close(); }); return null; }); jssc.start(); jssc.awaitTermination(); }}","link":"/2019/12/28/spark-编程常用例子（Java版）-1/"},{"title":"朴素贝叶斯","text":"概念 朴素贝叶斯概率研究的是条件概率，也就是研究的场景是在带有某些前提条件下，或者在某些背景条件的约束下发生的概率问题。 朴素贝叶斯公式 **P(A|B) * P(B) = P(B|A) * P(A)** P(A)：事件A发生的概率P(B|A)：事件A发生的情况下事件B发生的概率P(B)：事件B发生的概率P(A|B)：事件B发生的情况下事件A发生的概率 P(A|B) * P(B) = P(B|A) * P(A)，该公式如下图表示： 左圆圈：A发生的概率 右圆圈：B发生的概率 中间交集：A和B同时发生的概率 例子 为了进一步了解这个公式，我们举一个例子：从袋子里面取出一个球，一共取两次，计算一下有多少可能性。 **解：** 假设有两个事件A和B： A=取出蓝球 B=取出红球 第一次取出蓝球的概率是2/5，我们把这个概率叫P(A)，然后在A发生的情况下，再取出一个红球的概率是多少？ 即P(A) * P(B|A)为： (2/5) * (3/4) = 3/10 反过来计算： 第一次取出红球，第二次取出蓝球。同理，P(B)为3/5，P(A|B)为2/4。即P(B) * P(A|B)为： (3/5) * (2/4) = 3/10 符合公式： P(A|B) * P(B) = P(B|A) * P(A) 贝叶斯算法的使用 例子：疾病（高血压）预测根据基因片段信息预测患有高血压的概率。这个预测过程也是一个分类过程，训练样本是大量的个体基因信息和个人疾病信息。通过建模分析，得到一个基因片段和患有疾病之间的概率转换关系。如下表： 问题：如果有一个用户来做基因测试，测试结果的基因片段G1、G2分别为1、0，那么他患有高血压疾病的概率是多少？解：事件A：患高血压事件B：基因片段G1、G2分别为1、0那么要求的是事件B发生的条件下事件A发生的概率，也就是P(A|B)。根据贝叶斯公式：P(A|B) * P(B) = P(B|A) * P(A)得出：P(A|B) = (P(B|A) * P(A)) / P(B)根据样本数据求得：P(A)) = 3/10P(B|A) = 1/3P(B) = 3/10代入得出：P(A|B) = (1/3 * 3/10) / (3/10) = 1/3 Spark中使用朴素贝叶斯算法 Spark机器学习算法库提供了朴素贝叶斯算法： 1234//训练贝叶斯模型 val model = NaiveBayes.train(trainData)//预测val result = model.predict(testData)","link":"/2018/09/02/分类-朴素贝叶斯/"},{"title":"单一职责原则","text":"单一职责原则(Single Pesponsibility Principle, SRP)是SOLID五大设计原则最容易被误解的一个。也许是名字的原因，很多程序员根据SRP这个名字想当然地认为这个原则是指：每个模块应该只做一件事。但这只是一个面向底层实现细节的设计原则，并不是SRP的全部。对于SRP的描述是： 任何一个软件模块都应该只对某一类行为者负责。 大部分情况下，对于 “软件模块” 的简单定义就是一个源代码文件（如.java文件和.py文件等），也可以指一组紧密相关的函数和数据结构。对于 “行为者” 可以是该模块的用户和利益相关者，只要是这些人对系统进行的变更是相似的，都可以归为同一类行为者。如何去理解SRP这一原则，下面通过两个反面案例来详细说明。 反面案例1：重复的假象有一个员工管理模块Employee类，Employee类有三个函数：calculatePay()，reportHours()和save()。如图： 这三个函数分别对应的是三类不同的行为者，违反了SRP设计原则。 calculatePay()是计算工资函数：由财务部门制定，负责向CFO（首席财务官）汇报。 reportHours()是日常运营报告函数：由人力资源部门制定，负责向COO（首席营运官）汇报。 save()是持久化数据函数：由DBA制定，负责向CTO（首席技术官）汇报。 这三个函数被放在同一个源文件中，即同一个Employee类中，这样做实际上就等于使三类行为者的行为耦合在了一起，这有可能会导致CFO团队的需求变更影响到COO团队所依赖的功能。例如，calculatePay()函数和reportHours()函数使用了同样逻辑来计算员工的工时情况。开发者为了避免重复代码，通常会将该算法单独实现为一个名为regularHours()的函数。 那么，假设CFO团队需要修改工时的计算方法，而COO团队不需要这个修改，因为他们对工时的计算方法可能存在差异。这时，负责修改代码的开发者会注意到calculatePay()函数调用了regularHours()函数，当可能不会注意到该函数同时被reportHours()函数调用。 于是，该开发者就这样按照需求修改了工时的计算方法，同时CFO团队也验证了新算法可以正常工作。这项修改最终被成功部署上线了。但是，COO团队显然不知道这些变更事情的发生，在他们继续使用regularHours()产生报表时，随后就会发现数据错误的结果。 这类问题发生的根源就是因为我们将不同行为者所依赖的代码强凑合到了一起，对此，SRP强调这类代码一定要分开。 反面案例2：代码合并一个拥有很多函数的源代码文件必然会经历很多次代码合并，如果这些函数分别服务于不同的行为者，会给代码合并带来很多问题（如代码冲突、功能影响），虽然有svn和git代码托管工具，但是合并带来的问题还是不能得到有效解决。 例如，CTO团体的DBA决定要对Employee数据库表结构进行修改，与此同时，COO团队的HR需要修改员工工时报表的格式。这样，就很可能出现来自不同团队的开发者分别对Employee类进行修改的情况，导致各自的修改互相冲突，这就必须要进行代码合并。该例子中，这次代码合并不仅有可能让CTO和COO各自要求的功能出错，甚至连CFO原本正常的功能也可能受到影响。 这样的案例有很多，它们的一个共同点是，多人为了不同的目的修改了同一份源代码，这很容易造成问题的产生，而避免这种问题产生的方法就是将服务不同行为者的代码进行切分。 解决方案我们有很多方法可以解决上面例子中的问题，每一种方法都需要将相关的函数划分成不同的类。 1. 方法一 最简单直接的方法是将数据和函数分离。设计三个函数类共同使用一个数据类EmployeeData，每个函数类只包含与之行为者相关函数代码，这样就不存在互相依赖的情况了。如图： 这种方案的缺点在于：开发者需要在程序里处理三个函数类。 1. 方法二 在方法一的基础上，使用Facade设计模式（外观设计模式），把三个函数类中需要对外使用到的函数合并到一个Facade类中。如图： Facade设计模式的好处是：EmployeeFacade类的代码量较少，它仅仅包含了初始化和调用三个具体实现类的函数。 当然，也有些开发者更倾向于把最重要的业务逻辑和数据放在一起。对于上面例子，我们可以选择将最重要的函数保留在Employee类中，同时用这个类来调用其它没那么重要的函数。如图： 为了清晰的了解这些解决方案，每个函数类中都只有一个函数，事实上并非如此，因为计算工资、生成报表和持久化数据都比较复杂的过程，每个函数类都可能包含了许多私有函数。总而言之，上面的每一个类都分别容纳了一组作用于相同作用域的函数，且每个作用域各自的私有函数是互相不可见的。 总结单一职责原则主要讨论的是函数和类之间的关系——但是该原则在两个不同层面上会以不同的形式出现。在组件层面，可以将其称为共同闭包原则（Common Closure Principle），在软件架构层面，它是用于奠定架构边界的变更轴心（Axis of Change）。这两个原则会在后续的文章中深入讨论。","link":"/2019/04/15/单一职责原则/"},{"title":"线性回归","text":"概念 线性回归是利用被称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系建模的一个回归分析，且自变量和因变量的关系可用一条直线近似表示。 一元线性回归 只包含一个自变量和一个因变量 多元线性回归 两个或两个以上的自变量和一个因变量 例子-房价预测 如果房价只受房屋面积的影响，那么房价和房屋面积的关系可以使用一条直线来近似表示： 为了表示多元线性情况，现在增加一个影响房价的因素：房间数。如下表： 房屋面积 房间数 价格 2104 3 400 1600 3 330 2400 3 369 1416 2 232 3000 4 540 2000 3 420 1500 2 300 2900 3 560 1700 2 350 2900 4 510 自变量：房屋面积、房间数因变量：价格可以使用二元线性方程来表示： 线性方程-公式 线性方程公式 如果上述例子添加了一个自变量：房间数，线性方程公式： 因此一元或多元线性方程，统一写成如下格式（x0=1） 这样求线性方程则演变成了求方程的参数θ 线性方程-评估 评估：对于θ参数的求解，我们需要一个机制去评估θ是否最优，所以需要对做出的h函数进行评估。 损失函数：用于评估h函数的好坏，一般称为损失函数。 使用最小二乘法构建损失函数，损失函数如下： 损失函数描述的是预测值和真实值之差的平方和的平均值，也就是误差大小（1/2系数为了求导使得系数为1）。 线性方程-梯度下降 如何调整θ使得J(θ)取值最小有很多方法，这里介绍梯度下降法。 由之前所述，求θT的问题演变成了求J(θ)的极小值问题。而梯度下降法中的梯度方向由J(θ)对θ的偏导数确定，由于求的是极小值，因此梯度方向是偏导数的反方向， θ沿梯度下降变化值： α为步长（学习速率），当α过大时，有可能越过最小值，而α当过小时，容易造成迭代次数较多，收敛速度较慢。 （因为梯度指的是增长最快的方向，而往下降是减少最快的方向） α控制θ每次向J(θ)变小的方向迭代时的变化幅度。J(θ)对θ的偏导表示J(θ)变化最大的方向。假如数据集中只有一条样本，那么样本数量m为1，公式： 上述公式演变成： 当样本数量m不为1时，将公式J(θ) 带入求偏导，那么每个参数沿梯度方向的变化值由公式求得： 初始时θT可设为0，然后迭代使用该公式计算θT中的每个参数，直至收敛为止。由于每次迭代计算θT时，都使用了整个样本集，因此我们称该梯度下降算法为批量梯度下降算法(batch gradient descent)。 当样本集数据量m很大时，批量梯度下降算法每迭代一次的复杂度为O(mn)，复杂度很高。因此，为了减少复杂度，当m很大时，我们更多时候使用随机梯度下降算法(stochastic gradient descent),算法如下所示： 随机梯度下降在计算下降最快的方向时随机选一个数据进行计算，而不是扫描全部训练数据集，这样就加快了迭代速度。随机梯度下降并不是沿着J(θ)下降最快的方向收敛，而是震荡的方式趋向极小点。 注意，梯度下降可能得到局部最优，但在损失函数里已经证明线性回归只有一个最优点，因为损失函数J(θ)是一个二次的凸函数，不会产生局部最优的情况。（假设学习步长α不是特别大） spark实现-房价预测 数据集格式： 价格,房屋面积 房间数 使用spark机器学习算法： LinearRegression，具体代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package spark.ml.regression_analysisimport org.apache.spark.SparkConfimport org.apache.spark.ml.linalg.Vectorsimport org.apache.spark.ml.regression.LinearRegressionimport org.apache.spark.sql.SparkSession/** * 线性回归--梯度下降算法 */object LinearRegressionTest { def main(args: Array[String]) { //设置环境 System.setProperty(\"hadoop.home.dir\", \"F:/java_lib/hadoop-2.6.0-cdh5.11.1\") val conf = new SparkConf().setMaster(\"local\").setAppName(\"LinearRegressionTest\") val spark = SparkSession .builder() .config(conf) .getOrCreate() val sc = spark.sparkContext import spark.implicits._ //读取数据 val data = sc.textFile(\"D:/spark_ml_data/lpsa.txt\") .map{ line =&gt; val array = line.split(\",\"); (array(0).toDouble, Vectors.dense(array(1).split(' ').map(_.toDouble))) }.toDF(\"label\", \"features\") //val sampleData = data.randomSplit(Array(0.7D, 0.3D), 11L) //数据随机分成两份 //val train = sampleData(0) //训练集 val train = data //val test = sampleData(1) //测试集 val test = data data.select(\"features\").show(); //打印出特征 //创建模型 val lr = new LinearRegression() .setMaxIter(20) //设置最大迭代次数 .setRegParam(0.3) //正则化参数 .setElasticNetParam(1) //L1，L2混合正则化(aL1+(1-a)L2) val lrModel = lr.fit(train) //开始训练 // //输出模型相关参数 println(\"Coefficients: \" + lrModel.coefficients + \" Intercept: \" + lrModel.intercept); val trainingSummary = lrModel.summary println(\"numIterations: \" + trainingSummary.totalIterations) //执行迭代次数 println(\"objectiveHistory: \" + Vectors.dense(trainingSummary.objectiveHistory)) //每次迭代的(loss+regulation) println(\"RMSE: \" + trainingSummary.rootMeanSquaredError) //均方根误差 println(\"r2: \" + trainingSummary.r2) //正则化参数 trainingSummary.residuals.show //训练集的预测值和实际值的差 //预测测试数据 val prediction = lrModel.transform(test) //输出测试集的label和预测值 prediction.selectExpr(\"label\",\"features\",\"prediction\").show() spark.stop() }}","link":"/2018/10/28/回归-线性回归/"},{"title":"分而治之","text":"生活中经常看到跟排序相关的例子，如：微博中排名前10的热点话题，学校中考试平均成绩排名前3的班级，NBA中比赛胜率排名前8的球队等。排序可以让重点数据凸显出来，从而更容易得到关注。开发中，我们也常遇到排序的问题，如对一个集合a(1),a(2),a(3),…,a(N)进行排序，然后取出前10个最大值。 如果集合比较小，可以编写一段排序程序，并且在一台计算机上就可以快速执行完成。如果集合比较大（比如几十亿或几百亿个元素），这对一台计算机来说是非常吃力的，甚至无法完成计算。有一个比较好的方案是使用分治算法，先把一个大集合拆分为几个小集合，如一分为二，变为a(1),a(2),…,a(N/2)和a(N/2+1),a(N/2+2),…,a(N)，然后对每一半分别进行排序，并取出每一半集合中前10个最大值，最后一步是合并结果，把每一半集合前10个最大值合并到一个集合中（20个元素），之后对这个集合进行最后一次排序，取出最大的前10个值，这10个值就是整个大集合中排名前10的最大值。 上面例子使用到了分治的思路，从数据量、计算能力、内存容量、磁盘容量等维度上得到了扩展，把大数据进行拆分，充分使用多台计算机的计算能力和存储能力，这也是Google发明MapReduce程序的思路。 在架构设计中，分治思路同样可以发挥重要作用。分治可以理解为拆分，可以拆分数据库、拆分服务、拆分机器资源等，拆分的作用是可以使软件架构更易于扩展，以适应业务需求的快速变化（如用户量增长、请求量增长）。下面从三个方面来讲诉如何有效的扩展系统规模，以满足急剧膨胀的快速功能扩展和价值创造的需求。这三个方面用XYZ轴来表示，如图： X轴拆分 内容：通常叫水平扩展，通过复制服务或数据库以分散事务处理带来的负载。 场景： 1.数据库读写比例很高（可以达到5:1甚至更高）。 2.事务增长超过数据增长的系统。 用法： 1.复制服务，同时配置负载均衡器。 2.确保使用数据库的程序清楚读和写的区别。 原因：以复制数据和功能为代价获得事务的快速扩展。 要点：X轴拆分实施速度快、研发成本低，事务处理扩展效果好。然而，从运维角度来看，数据的运营成本比较高。 在应用系统开发中，关系型数据库是很多开发者选择的存储方式，在扩展问题的解决方案中最困难的部分经常是数据库或持久存储层。OLTP系统中大多数表都遵循为第三范式，数据库表的每一列都是不可分割的原子数据项，非关键字段完全依赖于主键，任何非关键字段不依赖于其它非关键字段。因为ACID（原子性、一致性、隔离性、持久性）属性，许多应用依靠数据库来支持和强制这些关系，导致这种数据库很难被拆分。 数据库扩展的一个技巧是利用大多数应用对数据库的读操作远远多于写操作。这种类型的系统可以通过复制只读数据的办法实现扩展。根据数据对时间的敏感性（数据必须是即时、实时、同步和完全正确的，这是理想状态，但是要实现这种系统的成本是昂贵的，因此，基于成本和实现复杂度的考虑，并不是所有的数据都必须是实时同步的），我们有几种不同的方法来分散只读数据。 一种方法是在数据库的前面加缓冲层。让读操作直接从缓冲层读取数据，而不用反复查询数据库。当读取的数据在缓存层不存在或已过期时，才触发查询数据库的操作，并更新最近查询的数据到缓冲层。这种方案目前有很多开源组件可以实现（如redis、memcache等），对频繁读操作的性能有显著提升。 还有一种方法是数据库复制。许多数据库通过主从方式来实现复制，主数据库是负责写入的主要事务型数据库，而从数据库是主数据库的只读副本。主数据库不断的跟踪数据的更新、插入、删除，并把记录存入一个二进制日志。从数据库从主数据库获取二进制日志后，在从数据库上重新执行这些命令。这是一个异步的过程，数据之间的时间延迟，取决于主数据库更新、插入、删除的数据量。该方案经常结合负载均衡器来实现，应用程序向负载均衡器发起读取请求，负载均衡器以轮询或者最少连接数的策略把请求传递给从数据库。目前数据库复制有两种常用方式，一种是以主主的概念进行复制，其中任意数据库都可以用来读和写，但是更建议使用另外一种方式，就是分离读和写数据库（也就是上面的方式，一个写数据库，多个读数据库），这样有助于消除混淆和避免数据库之间的逻辑争用。 我们把这种拆分称为X轴拆分（水平复制）。X轴拆分不仅可以应用于数据库，在可以应用在网络服务器和应用服务器的扩展中。这种水平复制的方式允许事务在系统之间均匀地分布以实现水平扩展，但是局限性也很明显，随着流量和数据的增大，业务越来越复杂，单纯的X轴拆分很快遇到性能瓶颈（内存、存储等资源的约束）。为了解决这些问题，我们需要引入Y轴和X轴扩展。 Y轴拆分 内容：Y轴扩展也称为功能（服务）或数据（资源）扩展。 场景： 1.数据之间的关联关系不是那么强的大型数据集。 2.需要拆分资源的大型复杂系统。 用法： 1.结合拆分功能（服务）和拆分数据（资源）。 2.沿着服务和资源定义的边界来拆分功能和数据。 原因：不仅允许应用事务和大型数据集有效扩展，也支持团队的有效扩展。 要点：Y轴扩展有益于故障隔离，也减低团队之间的沟通成本。 简单的说，Y轴拆分是通过在系统内部拆分不同的功能和数据从而实现扩展的方法。 首先，我们先通过功能（或服务）来拆分。拿电商系统来说，电商系统由登录、商品搜索、购物车、订单等功能模块组成。当分析登录和商品搜索这样迥然不同的功能时，这两种功能的边界划分和差异性比较明显，使得这种扩展方法更有效的实现。在登录的情况下，我们主要关注的是用户验证凭据和会话状态，因此需要缓存和该用户有关的交互数据。商品搜索关注的是寻找一个商品，关心的是用户的意图和商品。拆分这两类数据后，每类数据有属于各自的缓冲层和数据库持久层，将使我们能够在每个功能中投入更多的系统资源（如：cpu、内存、磁盘等），更充分地利用系统资源促使这两个功能模块有更快的响应速度。 其次，我们通过数据来拆分。继续电商系统为例，假设电商系统是由产品目录、产品库存、用户账户信息和营销信息等组成。我们可以根据数据按类型拆分，然后抽象出数据操作的接口，如创建、查询、更新、删除。 Y轴拆分对功能和数据的扩展最有价值，但同时对团队的扩展也很有用途。随着功能和数据的拆分，实现功能和操作数据的代码也会被拆分，这意味着技术团队也会被拆分为不同子团队，每个子团队只负责实现各自的功能模块，并在服务中建立和维护各自的接口（或API）。正因为拆分了功能和数据，应用系统才容易扩展事务。 Z轴拆分 内容：根据数据独特的属性（例如ID、地理位置、时间范围等）拆分。 场景：适用于非常大而且类似的数据集，如庞大的客户群、机器日志等。 用法：根据数据的属性对数据集进行分片（或分区）存储。 原因：快速增长的数据集，分片后有利于快速查询和故障隔离。 要点：Z轴拆分对于扩大数据基数的效果明显，非常适用于大型数据集上。 Z轴拆分通常被称为分片，是把一个数据集或一个服务分割成几块。一般情况下，这些块的大小比较平均，但也可能存在这些块大小不一，块之间大小差异比较大。生活中我们已经使用到了分片的思维，假如我们要把一堆书按照分类（文学、科技、数学等类别）分别装到不同的箱子中，并在每个箱子上贴上分类标签，这样，当我们要找一本 “红楼梦” 时，直接到贴有 “文学” 标签的箱子里找即可，而无需遍历所有的箱子，节省了寻找时间。 分片是根据数据的属性来拆分的，如：在电商系统中可以根据用户的地理位置对用户信息进行分片，在日志管理系统中可以根据日志产生日期对日志数据进行分片，如果数据没有明显的属性，可以对数据的ID取模或散列算法来产生分片。分片的最大用处是能够有效的分散数据集，提高数据查询响应速度。如果没有对数据分片，要查找一个数据时需要遍历一整个数据集而变得异常缓慢。 总结通常X轴扩展的成本最低，Y和Z轴的扩展方案相对复杂，但是更具有灵活性。结合这三种扩展方法，在大多数系统扩展时都会得到很好的效果。","link":"/2019/03/30/分而治之/"},{"title":"布隆过滤器","text":"算法背景 问题：在开发中，经常要判断一个元素是否在一个集合中。**实现方案：**编程中通常使用集合来存储所有元素，然后通过hash值来确定元素是否存在。如：java中的HashMap、HashSet等。优点：快速准确缺点：耗费存储空间 瓶颈：当集合比较小时，这个问题不明显当集合比较大时，散列表存储效率低的问题越明显 如：判断邮件地址是否是发送垃圾邮件的地址采用散列表：将每一个Email地址对应成一个8字节的信息指纹，然后存入散列表，由于散列表的存储效率一般只有50%，因此一个Email地址需要16个字节。一亿Email约1.6GB内存，存储几十亿个地址约上百GB的内存。 布隆过滤器–概念 布隆过滤器（英语：Bloom Filter）是1970年由布隆提出的。它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询效率都远远超过一般的算法，缺点是有一定的误判率和删除困难。 布隆过滤器—原理 布隆过滤器的原理是：当一个元素被加入集合时，通过K个散列函数将这个元素映射成一个位数组中的K个点，把它们置为1。检索时，我们只要看看这些点是不是都是1就（大约）知道集合中有没有它了：如果这些点有任何一个0，则被检元素一定不在；如果都是1，则被检元素很可能在。这就是布隆过滤器的基本思想。 布隆过滤器—缺点 bloom filter之所以能做到在时间和空间上的效率比较高，是因为牺牲了判断的准确率、删除的便利性。存在误判。可能要查到的元素并没有在容器中，但是hash之后得到的k个位置上值都是1。如果bloom filter中存储的是黑名单，可以通过建立一个白名单来存储可能会误判的元素。删除困难。一个放入容器的元素映射到bit数组的k个位置上是1，删除的时候不能简单的直接置为0，可能会影响其它元素的判断。可以采用Counting Bloom Filter（计数布隆过滤器），将标准Bloom Filter位数组的每一位扩展为一个小的计数器（Counter）。 布隆过滤器—实现 在使用bloom filter时，绕不过的两点：1）预估数据量n2）期望的误判率fpp 在实现bloom filter时，绕不过的两点：1）hash函数的选取2）bit数组的大小 对于一个确定的场景，我们预估要存的数据量为n，期望的误判率为fpp，然后需要计算我们需要的Bit数组的大小m，以及hash函数的个数k，并选择hash函数。 （1）Bit数组大小选择 根据预估数据量n以及误判率fpp，bit数组大小的m的计算方式：（2）哈希函数选择 由预估数据量n以及bit数组长度m，可以得到一个hash函数的个数k： 哈希函数的选择对性能的影响是很大的，一个好的哈希函数要能近似等概率的将字符串映射到各个Bit。选择k个不同的哈希函数比较麻烦，一种简单的方法是选择一个哈希函数，然后送入k个不同的参数。下面使用java来实现布隆过滤器。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224package com.blackhole.test.demo;import java.io.FileInputStream;import java.io.FileOutputStream;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;import java.io.Serializable;import java.math.BigInteger;import java.security.MessageDigest;import java.security.NoSuchAlgorithmException;import java.util.BitSet;import java.util.Date;import java.util.HashSet;import java.util.Set;/** * 布隆过滤器 * * @author Administrator * */public class BloomFilterDemo implements Serializable { private static final long serialVersionUID = -5221305273707291280L; private int dataCount; // 预期数要存的数据量 private double falsePositive; // 期望的误判率 private BitSet bits; // 位数组，使用BitSet实现 private int bitSize; // 位数组大小 private int hashFunctionCount; // 散列函数的个数 public BloomFilterDemo(int dataCount, double falsePositive) { super(); this.dataCount = dataCount; this.falsePositive = falsePositive; this.init(); } /** * 初始化 */ private void init() { this.bitSize = (int) this.getNumOfBits(this.dataCount, this.falsePositive); this.hashFunctionCount = this.getNumOfHashFunctions(this.dataCount, this.bitSize); this.bits = new BitSet(this.bitSize); } /** * 往布隆过滤器添加数据标记 * 如果不存在就进行记录并返回false，如果存在了就返回true * * @param data * @return * @throws NoSuchAlgorithmException */ public boolean add(String data) throws NoSuchAlgorithmException { int[] indexs = new int[this.hashFunctionCount]; // 先假定存在 boolean exist = true; int index; for (int i = 0; i &lt; this.hashFunctionCount; i++) { indexs[i] = index = hash(data, i); if (exist) { if (!this.bits.get(index)) { // 只要有一个不存在，就可以认为整个字符串都是第一次出现的 exist = false; // 补充之前的信息 for (int j = 0; j &lt;= i; j++) { //将对应的位设置为true this.bits.set(indexs[j], true); } } } else { //将对应的位设置为true this.bits.set(index, true); } } return exist; } /** * 检查数据是否存在 * * @param data * @return * @throws NoSuchAlgorithmException */ public boolean check(String data) throws NoSuchAlgorithmException { for (int i = 0; i &lt; this.hashFunctionCount; i++) { int index = hash(data, i); if (!this.bits.get(index)) { return false; } } return true; } /** * md5实现hash--目前测试，该hash算法的效果比较好 * @param message * @param funNum * @return * @throws NoSuchAlgorithmException */ private int hash(String message, int funNum) throws NoSuchAlgorithmException{ MessageDigest md5 = MessageDigest.getInstance(\"md5\"); message = message + String.valueOf(funNum); byte[] bytes = message.getBytes(); md5.update(bytes); BigInteger bi = new BigInteger(md5.digest()); return Math.abs(bi.intValue()) % this.bitSize; } /** * 获取Bit数组的大小 * * @param n * 预估要存的数据量 * @param p * 期望的误判率 * @return */ private long getNumOfBits(long n, double p) { if (p == 0) { p = Double.MIN_VALUE; } // -1 * (n * log(p)) / (log(2) * log(2)) return (long) (-n * Math.log(p) / (Math.log(2) * Math.log(2))); } /** * 获取hash函数的数量 * * @param n * 预估要存的数据量 * @param m * Bit数组的大小m * @return */ private int getNumOfHashFunctions(long n, long m) { // (m / n) * log(2) return Math.max(1, (int) Math.round((double) m / n * Math.log(2))); } /** * 持久化布隆过滤器对象 * * @param path */ public void saveFilterToFile(String path) { try (ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(path))) { oos.writeObject(this); } catch (Exception e) { throw new RuntimeException(e); } } /** * 读取布隆过滤器对象 * * @param path */ public static BloomFilterDemo readFilterFromFile(String path) { try (ObjectInputStream ois = new ObjectInputStream(new FileInputStream(path))) { return (BloomFilterDemo) ois.readObject(); } catch (Exception e) { throw new RuntimeException(e); } } public static void main(String[] args) { String file = \"d:\\\\bloomFilter.obj\"; try { createFilter(file); readFilter(file); } catch (NoSuchAlgorithmException e) { e.printStackTrace(); } } public static void createFilter(String file) throws NoSuchAlgorithmException { int count = 1000 * 10000; double fpp = 0.00001; System.out.println(\"start --- \" + new Date()); BloomFilterDemo bloomFilter = new BloomFilterDemo(count, fpp); int i = 0; while (i &lt; count) { String msg = \"时间：2018-10-01 10:00:00， 源IP：10.1.1.12，目标IP：192.1.1.205， 攻击类型：ddos攻击 -- \" + i; bloomFilter.add(msg); i++; } bloomFilter.saveFilterToFile(file); System.out.println(\"end --- \" + new Date()); } public static void readFilter(String file) throws NoSuchAlgorithmException { BloomFilterDemo bloomFilter = readFilterFromFile(file); System.out.println(\"bitSize: \" + bloomFilter.bitSize); System.out.println(\"hashFunctionCount: \" + bloomFilter.hashFunctionCount); System.out.println(\"start --- \" + System.currentTimeMillis()); int existCount = 0; for (int i = 0, size = 10 * 10000; i &lt; size; i++) { String msg = \"时间：2018-10-01 10:00:00， 源IP：10.1.1.12，目标IP：192.1.1.205， 攻击类型：ddos攻击 -- \" + i; msg += System.currentTimeMillis(); if (bloomFilter.check(msg)) { existCount++; } } System.out.println(\"end --- \" + System.currentTimeMillis()); System.out.println(existCount); }} 布隆过滤器—应用 常见的几个应用场景：（1）网页爬虫对URL的去重，避免爬取相同URL地址（2）反垃圾邮件，从数十亿个垃圾邮件列表中判断某邮箱是否垃圾邮箱（同理，垃圾短信）（3）缓存击穿，将已存在的缓存放到布隆中，当黑客访问不存在的缓存时迅速返回避免缓存及DB挂掉 布隆过滤器—公式推导 预估要存的数据量为：n期望的误判率为：PBit数组的大小为：mHash函数的个数为：k推导过程：1）对某一特定bit位在一个元素由某特定hash函数插入时没有被置为1的概率为：2）则k个hash函数都没有将其置为1概率为：3）如果插入了n个元素，都未将其置为1的概率为：4）反过来，则此位被置为1的概率为：5）一个不在集合中的元素，被误判在集合中的概率：6）根据自然常数公式： lim(1+1/x)^x, x→∞，得出：7）k为何值时可以使得误判率最低。设误判率为k的函数：8）设：9）则简化为：10）两边取对数：11）两边对k求导：12）下面求最值：==&gt;==&gt;==&gt;==&gt;==&gt;==&gt;==&gt;则误判率最低时，得出k值：13）把k代入误判率公式，得出：14）把k代入误判率公式，得出m值：","link":"/2018/09/08/布隆过滤器/"},{"title":"开闭原则","text":"开闭原则(Open Closed Principle, OCP)是Bertrand Meyer在1988年提出的，该设计原则认为： 一个设计良好的计算机系统应该易于扩展（对扩展开放），同时抗拒修改（对修改关闭）。 其实这也是我们设计软件架构的根本目的。如果对于小小的需求改动，而需要大幅度地修改整个软件架构，这种牵一发而动全身的设计显然是要避免的。下面，我们通过一个案例来对开闭原则做一些说明。 案例： 电商交易日志系统假设我们要设计一个电商交易日志系统，用于记录用户的交易行为记录。如：用户在某个电商平台购买商品，购买商品经过了以下步骤或页面操作，浏览商品目录 -&gt; 查看商品详情 -&gt; 加入购物车 -&gt; 提交订单 -&gt; 填写收货信息 -&gt; 支付等。该交易日志系统以日志的方式记录了用户购物的操作步骤，从而可以分析出某个用户购买行为存在的问题，这样就可以优化和调整购物操作流程，提高用户购物的转化率。 最开始用户量较少，存储方式也比较简单，选择了使用Mysql作为存储日志的数据库。用户的每次操作，都写入一笔记录到Mysql，然后使用SQL语句来分析用户购买行为。系统架构如下： 为了方面演示，这里只给出了两个重要组件：业务服务组件（电商业务逻辑处理组件）、日志记录组件（日志存储和分析组件），使用的是Mysql数据库。业务服务组件使用到了日志的存储和分析等操作，所以这里的业务服务组件依赖于日志记录组件。 随着电商系统的用户量的增加，日志量也会大幅度增加，使用Mysql存储和分析日志的瓶颈也越来越明显。传统的关系型数据库在横向扩展和大数据量处理方面比较吃力，为了解决这个问题，开发部门决定使用Hadoop来作为日志的存储和分析。 但是，由于之前的架构设计导致业务服务组件依赖于日志记录组件，要把Mysql替换为Hadoop的话，也需要修改业务服务组件代码。业务服务组件作为最核心的组件，现在为了修改日志记录组件而受到影响，这是开发者最不愿遇到的情况，最核心的组件应该是最稳定的和抗拒修改的。 为了避免核心组件受到不必要的影响，这里我们用到一个设计原则：如果A组件不想被B组件上发生的修改所影响，那么就应该让B组件依赖于A组件。修改后的架构如下： 所以现在的情况是，日志记录组件依赖于业务服务组件，在业务服务组件内部，使用接口的方式抽象出日志存储和分析的操作，日志记录组件负责实现这些接口即可。这样，以后就算还要使用其它方式的日志记录组件，只要实现业务服务组件定义的日志记录接口，而业务服务组件不需做修改。 以上就算我们在软件架构层次上对OCP这一设计原则的应用。开发者可以根据相关函数被修改的原因、修改的方式以及修改的时间来对其进行分组隔离，并将这些互相隔离的函数分组整理成组件结构，使得高阶组件（核心组件）不会因为低阶组件（辅助组件）被修改而受到影响。 总结OCP是我们进行系统架构设计的主导原则，其主要目标是让系统易于扩展，同时限制其每次被修改所影响的范围。实现方式是通过将系统划分为一系列组件，并将这些组件间的依赖关系按层次结构进行组织，使得高阶组件不会因低阶组件被修改而受到影响。","link":"/2019/05/06/开闭原则/"},{"title":"接口隔离原则","text":"接口隔离原则（Interface Segregation Principle，ISP）。ISP定义：客户端不应该依赖它不需要的接口；一个类对另一个类的依赖应该建立在最小的接口上。 案例说明直观认识ISP原则，直接上图： 在图中所描述的应用中，有多个用户需要操作Ops类。现在，我们假设User1类只需要使用op1，User2类只需要使用op2，User3类只需要使用op3。 在这种情况下，如果使用Java语言来实现，User1并不需要调用op2、op3方法，但是在代码层次上且与Ops类形成了依赖。这种依赖意味着我们对Ops类的op2方法所做的任何修改，即使不会影响User1的功能，也会导致Ops需要被重新编译和部署。 这个问题可以通过将不同的操作隔离成接口来解决，如下图： 同样，如果使用Java语言来实现，那么User1类会依赖于U1Ops接口，且U1Ops接口只有op1方法，但是User1类不会依赖Ops类，Ops类只需要负责实现U1Ops接口的方法即可。这样一来，我们之后对Ops类做的修改只要不影响到User1的功能，就不需要重新编译和部署User1了。 ISP于编程语言我们知道，一个算法、一个设计思路是跟具体的编程语言无关的。上面的例子很大程度上依赖于编程语言的一个特性，那就是导入需要的依赖代码，如Java语言的import，C语言的include等。而正是这些语句带来了代码之间的依赖关系，这也导致了某些模块需要被重新编译和重新部署。 对于Python和Ruby这样的动态语言来说，它们属于解释性语言，它们所依赖的代码会在运行时被推演出来，所以也就不存在重新编译和重新部署的必要性。这也是动态语言比静态语言更灵活、耦合度更松的原因。 当然，如果是这样的话，我们可能会误以为ISP只是一个与特定编程语言相关的设计原则，而非软件架构的设计原则，这就错了。 ISP于软件架构在一般情况下，任何层次的软件设计如果依赖于不需要的东西，都会是有害的。从代码层次来说，这样的依赖会导致不必要的重新编译和重新部署，从软件架构层次来说，问题也是类似的。 例如，我们在设计系统S（System）时，想要在系统中引入某个框架F（Framework），并假设框架F又绑定在一个特定的数据库D（DB）上，这样就形成了S依赖于F，F依赖于D的关系。如图： 在这种情况下，如果D中包含了F不需要的功能，那么这些功能同样也会是S不需要的。而我们对D中这些功能的修改将会导致F需要被重新部署，后者又会导致S的重新部署。更糟糕的是，D中一个无关功能的错误也可能会导致F和S运行出错。 总结ISP设计原则告诉我们：任何层次（代码和架构）的软件设计如果依赖了它并不需要的东西，就会带来意料之外的麻烦。","link":"/2019/05/17/接口隔离原则/"},{"title":"里氏替换原则","text":"里氏替换原则（Liskov Substitution Principle，LSP）是面向对象设计的基本原则之一。 里氏替换原则认为：任何基类（父类或接口）可以出现的地方，子类（实现类）一定可以出现。 LSP是继承复用的基石，只有当子类可以替换掉基类，软件单位的功能不受到影响时，基类才能真正被复用，而子类也能够在基类的基础上增加新的行为 为了更好理解这个原则，我们举两个例子（一个违反原则案例、一个符合原则案例）。 违反原则案例：长方形和正方形从长方形和正方形的特性中，我们知道正方形也是一种特殊的长方形。现在有一个需求是通过指定长方形（或正方形）的长和宽，求得该长方形的面积。由于正方形是一种特殊的长方形，开发者设计时把长方形作为父类，正方形作为子类并继承父类，如下图： 在这个案例中，Square类不应该是Rectangle类的子类，因为Rectangle类的高和宽可以分别修改，而Square类的高和宽则必须一同修改。由于User类直接对接和操作Rectangle类，因此会带来一些混淆。如下代码： Rectangle r = ...; r.setH(2); r.setW(5); assert(r.area() == 10);如果变量 r 指向的是Rectangle类对象，此断言是成立的，但是如果变量 r 指向的是Square类对象，则这个断言结果是不成立的。 想要避免这种违反LSP的行为，有一个方法是在User类中增加用于区分Rectangle和Square的检测逻辑（例如增加 if 判断逻辑）。但是这样一来，User类的行为又将依赖于它所使用的类，这两个类就不能互相替换了。 符合原则案例：排序算法假设现在有一个需求，要求对一组集合的数据进行排序。我们定义了一个Sort&lt;I&gt;接口，该接口有一个sort()排序方法，最开始我们使用了冒泡排序，所以定义了一个Sort接口的实现类BubbleSort。 集合数据比较少时，冒泡排序的处理速度还可以接受，但是随着集合数据的增加，冒泡排序的处理效率下降越明显。之后，我们打算使用快速排序替换掉冒泡排序，所以增加了一个排序实现类QuickSort，而User类直接操作的是Sort接口，因此User类不用做任何修改。如下图： 上述设计是符合LSP原则的，因为User类的行为并不依赖于Sort接口的任何一个实现类。如果由于需求需要，要使用堆排序替换掉快速排序，只需要增加一个排序实现类HeapSort即可。也就是说，这3个子类（实现类）的对象都是可以用来替换Sort基类（接口）对象的。 总结LSP是一种更广泛的、指导接口与其实现方式的设计原则。这里提到的接口可以有多种形式——可以是Java风格的接口，具有多个实现类；也可以像C++一样，父类指针指向子类对象；甚至可以是一个REST接口，多个服务程序只要遵循该REST的规范并响应同一个REST接口。 LSP应该被应用于软件架构层面，因为一旦违背了可替换性，该系统架构就不得不为此添加大量复杂的应对机制。","link":"/2019/05/09/里氏替换原则/"},{"title":"简单之美","text":"我们遇到问题时，时不时会谷歌或百度一下，谷歌或百度能在毫秒级时间内响应结果网页，我们不禁会想谷歌或百度是不是使用了什么特别复杂厉害的算法，才能到达如此的快速响应。我们先看看建立一个搜索引擎大致需要做这样的几件事：自动下载互联网上的网页；建立快速有效的索引；根据相关性对网页进行公平准确的排序。一个高质量的搜索引擎要实现这3件事都不容易，但是呢，搜索引擎的基本原理还是相对简单的。我们平时使用的计算机可以处理复杂的逻辑计算，但其底层采用的是最简单的二进制计数方法，它只有两个数字：0和1。二进制除了是一种计数方法外，它还可以表示逻辑的 “是” 与 “非”，这个特征在搜索引擎的索引中非常有用。布尔运算是针对二进制的，它很简单，可能没有比布尔运算更简单的运算了。搜索引擎的实现，从根本上就离不开布尔运算的框架。 布尔运算具有以下特点。布尔运算的元素只有两个：1（TRUE，真），0（FALSE，假）。基本的运算只有3种：与（AND）、或（OR）、非（NOT）。其对应的真值表如下： 与运算-真值表 AND | &nbsp;&nbsp; 1 &nbsp;&nbsp; | &nbsp;&nbsp; 0 &nbsp;&nbsp;—|—|—|—1 | &nbsp;&nbsp; 1 &nbsp;&nbsp; | &nbsp;&nbsp; 0 &nbsp;&nbsp;0 | &nbsp;&nbsp; 0 &nbsp;&nbsp; | &nbsp;&nbsp; 0 &nbsp;&nbsp; 或运算-真值表 OR | &nbsp;&nbsp; 1 &nbsp;&nbsp; | &nbsp;&nbsp; 0 &nbsp;&nbsp;—|—|—|—1 | &nbsp;&nbsp; 1 &nbsp;&nbsp; | &nbsp;&nbsp; 1 &nbsp;&nbsp;0 | &nbsp;&nbsp; 1 &nbsp;&nbsp; | &nbsp;&nbsp; 0 &nbsp;&nbsp; 非运算-真值表 NOT | &nbsp;—|—|—|—1 | &nbsp;&nbsp; 0 &nbsp;&nbsp;0 | &nbsp;&nbsp; 1 &nbsp;&nbsp; 在讲搜索引擎的基本原理之前，先看一个例子：如果你是图书馆管理员，如何才能让读者快速有效的找到一本书，你肯定会注意到，那就是为每一本书打上索引标签，我们不可能在图书馆书架上一本本地找，而是通过搜索索引标签定位到它的位置，然后直接去书架上拿。我们可以把一个网页比当做一本书，那接下来就要为网页创建索引，最简单的索引结构是用一个很长的二进制数表示一个关键字是否出现在网页中，每一位对应一个网页，1 代表相应的网页包含这个关键字，0 代表没有。比如关键字 “简单” 对应的二进制数是 0100000101…，表示第二、第八、第十个网页包含这个关键字。同样，假定 “架构” 对应的二进制数是 0100110011…，那么要找到同时包含 “简单” 和 “架构” 的网页时，只需要将这两个二进制数进行布尔运算AND。根据上面的真值表，可以得到结果为 0100000001…，表示第二、第十个网页满足需求，而计算机做布尔运算的速度是非常快的，所以我们使用搜索引擎时，能够快速的搜索到想要的结果。然而互联网上的网页数量是庞大的，由于这些二进制数中的绝大部分位数都是 0，只需要记录那些等于 1的位数即可。这样，搜索引擎的索引就变成了一张大表：表的每一行对应一个关键字，每个关键字后面跟着一组二进制数字，包含了该关键字的网页索引号（这种索引的结构设计，也称 “倒排索引”）。当然，要设计和实现一个高质量的搜索引擎远还是非常复杂的，但是原理上还是相对简单的，即等价于布尔运算。 布尔运算非常简单，但是对搜索引擎的实现有着重大意义，这让我们感受到了简单之美。在实际开发产品的解决方案中，首选的做法是采取简单的实现方式。问题的复杂度要与解决问题的方法及成本相匹配，尽量的保持问题解决方案的简单，我们认为复杂问题只是一系列小而简问题的集合。本章和大家探讨一下如何把大事化小，从而达到事倍功半。下面从6个规则来分别讲诉“如何简化”，有的规则比较宏观，适用多种场景，有的规则比较微观，只适用特定场景。 规则1-避免复杂设计 内容：在设计中要警惕复杂的解决方案。 场景：适用于任何项目的设计中，特别是庞大而复杂的系统或项目。 用法：当设计出解决方案时，通过测试同事是否能够容易理解和实现你的方案，来验证是否存在复杂设计。 原因：复杂的解决方案在实现和维护上成本高（包括人力成本和时间成本）。 要点：过度复杂的系统限制了可扩展性。简单的系统易维护、易扩展、成本低。 关于复杂设计，主要包括两方面，一是系统或项目结构的复杂性，一是业务逻辑的复杂性。 （1）结构复杂的系统可以归为具有两个特点：组成复杂系统的组件数量过多，同时这些组件之间的关系依赖过于复杂。如果组件的抽象程度和具体程度没有设计好，很容易面临牵一发而动全身的局面。结构复杂有如下缺点：&nbsp;&nbsp; a、组件越多，某个组件出现故障的概率也就越大，从而越容易导致系统故障。&nbsp;&nbsp; b、组件关系依赖越复杂，当某个组件有变动时，会影响所有和它有关系的其它组件，同样这些被影响的组件也会递归性的影响更多的组件。&nbsp;&nbsp; c、定位一个复杂系统的问题比简单系统更加困难，在排查问题时，需要对所有相关组件都要逐一排查，降低解决问题的效率。 （2）逻辑复杂的系统的一个典型特点是单个组件承担了太多的功能。以电商系统为例，常见的功能有：商品管理、库存管理、订单管理、用户管理、支付、物流、客服……如果把这些功能都放在一个组件上实现，可想而知，这个组件简直就是一个巨型、笨重的巨无霸。逻辑复杂的另外一个典型特点是采用了复杂的处理算法，复杂处理算法导致的问题主要是理解上，从而导致难以实现、难以维护。 复杂设计是可扩展性的大敌之一。设计的越复杂，问题也就越多，越不易于后续的扩展。有一个好方法可以验证解决方案是否过于复杂，把解决方案展现给不同的技术团队，如果每个技术团队都能够轻松理解方案，并可以向其他人描述该方案，那就可以采用该方案，如果有其中一个技术团队表示不理解，那就要针对该方案是否过于复杂而进行辩论或调整。所以架构设计时谨遵一句话：Keep It Simple! 如果使用简单的解决方案就可以满足需求，那就选择简单方案。 规则2-方案中包含可扩展 内容：提供及时可扩展性的DID方法。 场景：适用于任何项目的设计中，是保证可扩展性的最经济有效的方法（资源和时间）。 用法： 1.Design（D）设计20倍的容量。 2.Implement（I）实施3倍的容量。 3.Deploy（D）部署1.5倍的容量。 原因：DID为产品扩展提供了经济、有效、及时的方法。 要点：在项目设计早期考虑扩展可以帮助团队节省时间和金钱，在需求发生大约一个月前实现功能，在客户蜂拥而至的前几天部署。 我们经常遇到的一个问题是 “什么时候该在扩展上投入”，有些轻率的答案是，最好是在需要的前一天投入和部署，因为这样才能使得这笔投资的价值最大化，用最少的钱换取最大的收益，有助于公司财务和股东利益最大化。但是很遗憾，这是不现实的，及时投入和部署根本就不可能，而且会带来很大风险，影响及时投入和部署的因素有：无法确定的具体时间、无法预测的具体流量、无法感知的竞争波动等。我们虽然不能做到及时投入和部署，但是我们尽可能的靠近 “实时”。这里采用AKF合伙公司在思考可扩展性时用的DID（设计-实施-部署）方法。这3个步骤和总所周知的认知阶段一致：思考问题和设计方案，为方案构建系统和代码实现，最后是安装和部署。表：扩展的DID过程 &nbsp; 设计 实施 部署 扩展的目标 20倍 ~ 无限 3 ~ 20倍 1.5 ~ 3倍 智力成本 高 中 低到中 工程成本 低 高 中 资产成本 低 低到中 高到很高 总成本 低到中 中 中 （1）设计（D，Design） 在项目的设计阶段，讨论和设计很明显要比我们在代码中实现该设计的成本更低，我们可以未雨绸缪，讨论好和草拟好如何扩展平台的设计。例如，最开始我们明显不想部署比现场的生产环境高出10倍、20倍甚至100倍的容量（这里的容量指的是系统的可扩展程度，包括：流量资源、计算资源、存储资源等），由于在设计阶段考虑扩展维度的成本比较低，建议在DID的D（设计）阶段聚焦在扩展到20倍到无限大之间。我们需要时刻进行 “头脑风暴”来思考和讨论 “大问题”，所以我们的智力成本很高。然而，我们不编写代码和部署昂贵的系统，所以工程成本和资产成本较低。最后我们归结到D阶段的总成本是低到中，所以我们应该好好利用该方法，在设计阶段发现和确定需要扩展的部分。 （2）实施（I，Implement） 设计方案确定后，我们可以开始编写代码实现设计。实施阶段是实现具体细节的阶段，建议在DID的I（实施）阶段实现当前规模的 3 ~ 20倍。“规模”在这里是指扩大最大瓶颈的系统组件，用以实现业务目标。可能在有些情况下，把一个组件的规模扩大100倍的成本与扩大20倍没有区别，如果是这样，我们可以一次完成该扩展的改变，而并非反复折腾。如随着用户数量的逐渐增大，平台需要基于用户属性（用户ID）取模来进行分库存储，把用户分散到多个（N个）数据库。我们可以定义一个可配置的变量User_Mod，其取值范围是1（现在） ~ 5（预测3年内使用5个数据库存储），如果经过1年后，发现用户的增长速度大于预期值，则可以把User_Mod的取值范围改为1 ~ 10，这样就达到可以轻易的扩展规模。这种改变的实施成本确实不会随着规模N的的变化而变化，这类改变以工程成本计算很高，以智力成本计算中等，以资产成本计算低到中。最后我们归结到I阶段的总成本是中，该阶段主要涉及到代码的具体实现，需要把代码架构涉及为易于扩展，尽可能的使用可配置的方式达到扩容。 （2）部署（D，Deploy） DID的最后阶段是部署（D）。仍以前面的用户取模为例，我们希望以及时的方式部署系统，没有理由因为资产空闲而稀释股东的价值。该阶段的资产成本往往是最高的，部署相当于现有规模100倍的系统将会使很多公司破产，建议在该阶段的扩容提高到1.5 ~ 3倍（根据实际情况而定，如果是超高增长的公司，可以考虑提高到5倍）。然而，我们没有必要把33%甚至更多的的资源放在那里等待突然爆发的用户活动，云计算就是一个用来应付突发请求的不错选择。记住，扩展要具有弹性，它既可以扩展也可以收缩，随着用户流量的变动而调整容量。 对于可扩展的设计和思考的成本相对较低，因此应该经常进行。对于实施阶段的成本是中等，应该考虑到代码架构的实现和扩容，尽可能的实现可配扩容。最后，就是部署阶段的基础设施的扩容，根据需要提前做好设备的订购准备，建议使用云服务，在接近所需和接近实时的情况下，快速的把所有服务部署运行起来。 规则3-3次简化方案 内容：在设计复杂系统时，从项目的范围、设计和实施角度简化方案。 场景：当设计复杂系统或产品时，面临着技术和计算资源的限制。 用法： 1.采用帕累托（Pareto）原则简化范围。 2.考虑成本优化和可扩展性来简化设计。 3.依靠他人的经验来简化实施。 原因：只聚焦“不过度复杂”，并不能解决需求或历史发展与沿革中的各种问题。 要点：在产品研发的各个阶段都需要做好简化。 规则1是关于抑制某些方案过于复杂的冲动，而规则3这是关于采用什么方法来进一步简化方案，主要从3个方法来简化：如何简化方案范围？如何简化方案设计？如何简化方案实施？ （1）如何简化方案范围对于这个简化问题的答案就是不断的应该用帕累托原则，也就是二八定理。如：“收益的80%来自于20%的工作”，也就是说“你收益的80%是由哪些20%的功能实现的？”，做的少同时取得显著的效益。也就是说要学会做“减法”，删除掉一些不必要的功能，最开始设计项目和产品时，避免设计的又全又大，把所有精力和注意力都放在最主要的功能上面。这样的好处是系统将会减少功能之间的依赖关系，可以更高效和更高本益比的进行扩展。借用马蒂·凯甘的理念 “最小化可行产品”，秉持着 “You Can Always Do Less”来简化方案范围。 （2）如何简化方案设计简化设计于过度设计的复杂性密切相关，消除复杂性也就是忽略无关要紧的模块。简化设计是基于具体场景而定的，如：为了让请求及时响应，可以先在缓存中获取数据，缓存中没有再从数据库中获取，这就是缓存的应用。为了统计某个文件的单词频率，看起来可以使用流行的MapReduce算法来处理，但是如果只是一个小文件，就不必要使用重量级的MapReduce算法，直接使用一个简单的程序来实现更有道理。简单的说，简化设计的步骤要求易于理解、低成本、高效益和可扩展的方式来完成工作。 （3）如何简化方案实施方案实施是方案的代码实现，这个也是基于具体场景来定的。如：某个问题，是使用递归还是循环来实现？某个解决方案，是自己研发还是使用开源项目？这些问题的答案都指向了一个公共主题：如何利用其它经验和已存在方案来简化方案实施。考虑到成本问题，我们应该首先寻找被广泛采用的开源项目满足需求，如果这些不存在，应该寻找本公司或其它组织内有类似解决方案经验的人来解决问题，如果都不存在，才考虑是否要自己研发来解决问题。 规则4-减少域名解析 内容：从用户角度减少域名解析次数。 场景：对性能敏感的所有网页。 用法：尽量减少下载页面所需的域名解析次数，但要保持于浏览器的并发连接数平衡。 原因：域名解析耗时而且大量域名解析会影响用户体验。 要点：减少对象、任务、计算等是加快页面加载输的的好办法，但要权衡浏览器的并发连接数。 本规则让我们在用户的浏览器上考虑简化的问题。当你浏览一个网页时，使用浏览器的开发者工具（火狐firebug、chrome开发工具等）就会观察到一些有趣的结果。你会发现网页上的对象（html、图像、js、css等）的下载时间各不相同，而有一个额外的步骤就是域名解析。域名服务系统（DNS）是互联网中最重要的部分之一，它的作用是把一个域名（ www.google.com ）解析为对应的IP（x.x.x.x）。事实上，几乎所有的网页都是由许多不同的对象组成，浏览器就是基于此，通过并发连接同时下载过个对象。浏览器对每个服务器或网关代理的最大持久并发连接数有限制。根据HTTP/1.1 RFC协议，这个最大值应该设置为2。但是，现在许多浏览器都忽略此限制，把最大值设置为6或者更大。 由此可见，网页上的域名解析次数越少，网页下载的性能就越好。但是把所有的对象都放在同一个域中会带来问题，那就是浏览器对同一个域的并发连接数做了限制。如何权衡域和浏览器的并发数量，我们在下一个规则中来探讨这个问题。 规则5-减少页面目标 内容：尽可能的减少网页上的对象数量。 场景：对性能敏感的所有网页。 用法：1.减少或者合并对象，但要平衡最大并发连接数。 2.寻找机会减轻对象的重量。 3.不断测试确保性能的提升。 原因：对象数量的多少直接影响网页的下载时间。 要点：对象数量和浏览器并发连接数的平衡是一门科学，需要不断测量和调整。这是在易用性、可用性和性能之间的平衡。 2009年谷歌发布了一份白皮书，声称测试表明搜索延迟如果增加400毫秒将每日搜索量减少大约0.6%，可以看出网页下载时间对于保住流量的重要性。一个网页包含许多不同的对象（html、图像、js、css等），浏览器会分别独立下载这些对象，而且是以并发的方式下载的。改进网页的第一个方法是减少对象的数量。大多数网页中下载比较耗时的是图像对象，如果一个网页的图像对象过多，如上百个，该网页的平均响应时间超过12秒，那该网站已经面临着损失有价值的流量。一个好的方法就是使用图像精灵，把一些小图像组合成一个大的图像，通过CSS来单独显示其中的任何一个图像，这样图像的请求数量会显著减少。当然，不是说把网页对象删除的越多越好，必须在把重要信息传达给客户的前提下，做好简化工作。 从上一个规则中，我们知道浏览器支持从单一域名同时下载多个对象，如果所有的元素都集中到一个对象中，那么浏览器的并发下载的能力就无法起作用，我们需要考虑把这些对象拆分为多个较小的对象，以便同时下载。 流量器对每个域名服务的并发连接数存在的资源限制。如果网页上的多有对象都来自单个域名，那么浏览器设置的最大连接数是多少，支持同时下载的对象就是多少。因此，最好把网页的内容拆分为合适多的对象数，以便利用浏览器的并发下载功能。有一种技巧就是把不同网页对象分别存储在不同的子域名上（例如：google.com, static1.google.com, static2.google.com）。浏览器把这些当成不同的域名对待，允许每个子域名拥有自己的最大并发连接数。 理想的网页应该有多少个对象以及多大重量没有绝对的答案。提高网页性能和可扩展的关键是测试。这需要考虑很多因素，如尽可能的保持网页足够轻，在网页必须很重的情况下，采用Gzip压缩以减轻网页的传输压力，利用浏览器缓存、缩小文件、延迟加载等优化技术。总之，页面上的对象越少性能越好，当必须与其它因素平衡，通过不断测试和调整来优化达到最佳效果。 规则6-采用同构网络 内容：确保交换机和路由器源于同一供应商。 场景：设计和扩大网络。 用法： 1.不要混合使用来自不同OEM的交换机和路由器。 2.购买或者开源的其它网络设备（防火墙等）。 原因：不同供应商或品牌的设备存在兼容问题，混合使用导致不可用性和限制扩展性。 要点：异构网络设备容易导致可用性和扩展性问题，选择单一供应商。 几乎所有的网络设备供应商都声称在他们的设备上实现了标准协议，允许来自不同供应商的设备之间进行通信，但是许多供应商也在设备上实现了专有协议，如思科的增强型内部网关路由协议。做过网站开发的就知道同一个网页在IE、Firefox和Chrome等不同浏览器的上的呈现效果就会不一样，这就是按照同一标准的不同实现上会有多么的不同了。因此，建议使用同一供应商的网络设备，这样就避免出现兼容性的问题。 总结本章主要围绕简化这个主题来探讨了6个简化规则，如何防止复杂性（规则1），如何尽早考虑扩展（规则2），如何从初始需求到最终实施做出简化（规则3），如何减少域名解析次数和减少页面对象数量（规则4和5），如何保持网络的简单和同构（规则6）。","link":"/2019/09/24/简单之美/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","link":"/tags/elasticsearch/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"flume","slug":"flume","link":"/tags/flume/"},{"name":"game","slug":"game","link":"/tags/game/"},{"name":"spark","slug":"spark","link":"/tags/spark/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"算法","slug":"算法","link":"/tags/算法/"},{"name":"机器学习","slug":"机器学习","link":"/tags/机器学习/"},{"name":"架构","slug":"架构","link":"/tags/架构/"},{"name":"回归","slug":"回归","link":"/tags/回归/"}],"categories":[{"name":"elasticsearch","slug":"elasticsearch","link":"/categories/elasticsearch/"},{"name":"linux","slug":"linux","link":"/categories/linux/"},{"name":"flume","slug":"flume","link":"/categories/flume/"},{"name":"game","slug":"game","link":"/categories/game/"},{"name":"spark","slug":"spark","link":"/categories/spark/"},{"name":"java","slug":"java","link":"/categories/java/"},{"name":"机器学习","slug":"机器学习","link":"/categories/机器学习/"},{"name":"架构","slug":"架构","link":"/categories/架构/"},{"name":"算法","slug":"算法","link":"/categories/算法/"}]}